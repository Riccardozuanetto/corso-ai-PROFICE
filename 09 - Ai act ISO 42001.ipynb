{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b4b616-8144-4b19-b83b-a4d674a0bce7",
   "metadata": {},
   "source": [
    "# AI Act – Punto 1: Classificazione per rischio (spiegata semplice)\n",
    "\n",
    "## 1) Le 4 categorie in 1 riga ciascuna\n",
    "\n",
    "* **Rischio inaccettabile → vietato.** Es.: social scoring, IA manipolativa.\n",
    "* **Alto rischio → regolato.** Regole e controlli stringenti.\n",
    "* **Rischio limitato → trasparenza.** Devi avvisare gli utenti (chatbot, deepfakes).\n",
    "* **Rischio minimo → libero.** In genere niente obblighi specifici AI Act (es.: videogiochi, filtri spam).\n",
    "\n",
    "## 2) Come capire in quale categoria sei (check veloce)\n",
    "\n",
    "1. **È inaccettabile?**\n",
    "   Se il sistema fa pratiche vietate (p.es. social scoring/manipolazione), **non può essere messo sul mercato UE**. Stop.\n",
    "2. **Incide su ambiti critici?**\n",
    "   Se l’IA è usata in contesti sensibili (es. decisioni che impattano sicurezza/diritti, componenti di sicurezza di prodotti, ecc.), probabile **alto rischio** → serviranno requisiti rigorosi (gestione rischi, dati, documentazione, ecc.).\n",
    "3. **Interagisce “in faccia” all’utente o crea media sintetici?**\n",
    "   Chatbot, avatar, sintetici/deepfake → **rischio limitato** → **obbligo di avviso** (“stai parlando con un’IA”, “immagine generata”).\n",
    "4. **Tutto il resto?**\n",
    "   **Rischio minimo.** Nessun obbligo specifico AI Act (restano le leggi generali).\n",
    "\n",
    "## 3) Esempi rapidi (indicativi)\n",
    "\n",
    "* **Inaccettabile:** sistema che attribuisce punteggi sociali alle persone → **vietato**.\n",
    "* **Alto rischio:** IA che aiuta a selezionare candidati per un lavoro → **regolato**.\n",
    "* **Limitato:** assistente virtuale sul sito che risponde in chat → **devi avvisare** che è un’IA.\n",
    "* **Minimo:** filtro antispam della posta → **nessun obbligo AI Act**.\n",
    "\n",
    "## 4) Da ricordare\n",
    "\n",
    "* Il grosso degli obblighi pratici ricade sui **fornitori di sistemi ad alto rischio** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53828d2-7fcf-4520-bf58-209fb0792841",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# AI Act – Sistemi di IA Proibiti (Capitolo II, Art. 5)\n",
    "\n",
    "### 1. Principio base\n",
    "\n",
    "Alcuni sistemi di IA sono considerati **ad alto pericolo per i diritti fondamentali** → quindi **vietati in assoluto nell’UE** (salvo poche eccezioni molto specifiche per la sicurezza pubblica).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Quali sistemi sono vietati?\n",
    "\n",
    "1. **Manipolazione occulta**\n",
    "\n",
    "   * Tecniche subliminali, manipolative o ingannevoli che distorcono il comportamento delle persone e compromettono decisioni consapevoli → **vietate se causano danno significativo**.\n",
    "   * Esempio: un’app che spinge inconsciamente i bambini a spendere soldi in giochi online.\n",
    "\n",
    "2. **Sfruttamento delle vulnerabilità**\n",
    "\n",
    "   * Usare IA per approfittarsi di debolezze legate ad **età, disabilità o condizione socio-economica**.\n",
    "   * Esempio: pubblicità mirata a persone anziane per spingerle a comprare prodotti inutili o dannosi.\n",
    "\n",
    "3. **Categorie biometriche sensibili**\n",
    "\n",
    "   * Sistemi che inferiscono da dati biometrici attributi sensibili: **razza, opinioni politiche, sindacato, religione, filosofia, vita sessuale, orientamento sessuale**.\n",
    "   * Eccezioni: solo per etichettare dataset biometrici legittimamente acquisiti o usati da forze dell’ordine con limiti precisi.\n",
    "\n",
    "4. **Social scoring**\n",
    "\n",
    "   * Valutare/classificare persone o gruppi in base a comportamento sociale o caratteristiche personali, con effetti negativi → **vietato**.\n",
    "   * Esempio: punteggio sociale alla “cinese” che decide accesso a servizi.\n",
    "\n",
    "5. **Previsioni di criminalità**\n",
    "\n",
    "   * Sistemi che stimano la probabilità che qualcuno commetta reati **solo con profiling o tratti di personalità**.\n",
    "   * Vietato, tranne come supporto a valutazioni umane basate su fatti oggettivi e verificabili.\n",
    "\n",
    "6. **Database facciali da scraping massivo**\n",
    "\n",
    "   * Raccolta indiscriminata di immagini facciali da Internet o telecamere di sorveglianza per creare database → **vietata**.\n",
    "\n",
    "7. **Riconoscimento emozioni in lavoro/scuola**\n",
    "\n",
    "   * Vietato inferire emozioni di dipendenti o studenti.\n",
    "   * Eccezioni: **scopi medici o di sicurezza**.\n",
    "\n",
    "8. **Riconoscimento biometrico remoto in tempo reale (RBI)**\n",
    "\n",
    "   * In spazi pubblici, da parte delle forze dell’ordine, è **vietato** salvo 3 eccezioni:\n",
    "\n",
    "     * Ricerca persone scomparse, vittime di rapimento o tratta.\n",
    "     * Prevenire minacce imminenti alla vita o attacchi terroristici.\n",
    "     * Identificare sospetti in **reati gravi** (omicidio, stupro, terrorismo, traffico di droga o armi, criminalità organizzata, crimini ambientali).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Note specifiche sull’uso del riconoscimento biometrico remoto\n",
    "\n",
    "* **Condizione di necessità**: deve essere usato solo se l’assenza dell’IA causerebbe gravi danni.\n",
    "* **Diritti delle persone**: bisogna garantire tutela delle libertà fondamentali.\n",
    "* **Obblighi procedurali per la polizia**:\n",
    "\n",
    "  1. **Valutazione d’impatto sui diritti fondamentali** → da completare prima dell’uso.\n",
    "  2. **Registrazione nel database UE** → obbligatoria; se urgenza, possibile uso immediato ma registrazione subito dopo.\n",
    "  3. **Autorizzazione preventiva** da parte di un’autorità giudiziaria o amministrativa indipendente.\n",
    "\n",
    "     * In caso di urgenza, si può procedere subito ma l’autorizzazione va chiesta entro 24 ore.\n",
    "     * Se rifiutata, il sistema va **disattivato immediatamente** e tutti i dati cancellati.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. In sintesi\n",
    "\n",
    "Il messaggio è chiaro: **l’UE vieta sistemi di IA che manipolano, discriminano, sorvegliano in modo massivo o predicono comportamenti pericolosi senza basi oggettive**.\n",
    "Le **eccezioni** (soprattutto per polizia e sicurezza) hanno **paletti procedurali molto rigidi** per proteggere i diritti dei cittadini.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1cba24-6f05-4bbc-a884-c92be83d38fc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# AI Act – Sistemi di IA ad Alto Rischio (Capitolo III)\n",
    "\n",
    "## 1. Quando un sistema è “ad alto rischio” (Art. 6)\n",
    "\n",
    "Un’IA è **High Risk** se rientra in uno di questi casi:\n",
    "\n",
    "1. **Prodotti soggetti a normative UE (Allegato I):**\n",
    "\n",
    "   * L’IA è usata come **componente di sicurezza** di un prodotto già regolato da altre leggi UE (es. dispositivi medici, auto, macchinari industriali).\n",
    "   * In più, quel prodotto deve essere sottoposto a **valutazione di conformità da terze parti** secondo le leggi UE di riferimento.\n",
    "\n",
    "2. **Casi d’uso elencati nell’Allegato III**, tranne quando:\n",
    "\n",
    "   * fa solo un compito tecnico e ristretto;\n",
    "   * migliora un’attività già conclusa da un umano;\n",
    "   * individua pattern o anomalie **senza sostituire il giudizio umano**;\n",
    "   * prepara dati/valutazioni da usare poi da un umano in un processo decisionale.\n",
    "\n",
    " Nota: Se il sistema fa **profilazione di individui** (cioè valuta dati personali per dedurre caratteristiche di vita, salute, lavoro, interessi, ecc.), è **sempre** considerato ad alto rischio.\n",
    "\n",
    " Se un fornitore pensa che il suo sistema **non sia ad alto rischio pur rientrando nell’Allegato III**, deve comunque **documentare questa valutazione** prima di immetterlo sul mercato.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Obblighi per i fornitori di IA ad alto rischio (Art. 8–17)\n",
    "\n",
    "Chi sviluppa un sistema ad alto rischio deve:\n",
    "\n",
    "1. **Gestione dei rischi:**\n",
    "   Avere un processo continuo per individuare, valutare e ridurre i rischi lungo tutto il ciclo di vita.\n",
    "\n",
    "2. **Governance dei dati:**\n",
    "   Usare dataset di training, validazione e test **rilevanti, rappresentativi, completi e con il minor numero possibile di errori**.\n",
    "\n",
    "3. **Documentazione tecnica:**\n",
    "   Redigere un fascicolo che dimostri la conformità e sia disponibile per le autorità.\n",
    "\n",
    "4. **Registrazione eventi:**\n",
    "   Il sistema deve poter **registrare automaticamente eventi** rilevanti per la sicurezza e modifiche importanti.\n",
    "\n",
    "5. **Istruzioni per l’uso:**\n",
    "   Fornire linee guida chiare a chi userà l’IA, così che possa rispettare le regole.\n",
    "\n",
    "6. **Supervisione umana:**\n",
    "   L’IA deve essere progettata per consentire un controllo e intervento umano effettivo.\n",
    "\n",
    "7. **Affidabilità:**\n",
    "   Raggiungere livelli adeguati di **accuratezza, robustezza e cybersecurity**.\n",
    "\n",
    "8. **Sistema di qualità:**\n",
    "   Implementare un **Quality Management System** che garantisca il rispetto di tutti i requisiti.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Allegato III – Casi d’uso ad alto rischio\n",
    "\n",
    "### a) Biometria (non vietata ma ad alto rischio)\n",
    "\n",
    "* Riconoscimento biometrico remoto (tranne la semplice verifica identità).\n",
    "* Categorizzazione biometrica che deduce attributi protetti.\n",
    "* Riconoscimento emozioni.\n",
    "\n",
    "### b) Infrastrutture critiche\n",
    "\n",
    "* IA che gestisce sicurezza di infrastrutture digitali, traffico stradale, reti di acqua, gas, elettricità, riscaldamento.\n",
    "\n",
    "### c) Istruzione e formazione\n",
    "\n",
    "* IA che decide accesso/ammissione a scuole o corsi.\n",
    "* Valutazione degli studenti, assegnazione a livelli, monitoraggio nei test.\n",
    "\n",
    "### d) Lavoro e occupazione\n",
    "\n",
    "* Reclutamento (selezione, screening CV, annunci mirati).\n",
    "* Decisioni su promozioni/licenziamenti.\n",
    "* Monitoraggio e valutazione dei dipendenti.\n",
    "\n",
    "### e) Servizi essenziali (pubblici e privati)\n",
    "\n",
    "* Valutazione per accesso a benefici sociali.\n",
    "* Valutazione di **creditworthiness** (esclusa la sola rilevazione di frodi).\n",
    "* Gestione chiamate di emergenza (priorità a polizia, ambulanze, pompieri).\n",
    "* Assicurazioni salute/vita → risk assessment e pricing.\n",
    "\n",
    "### f) Forze dell’ordine\n",
    "\n",
    "* Valutare rischio di diventare vittima di un reato.\n",
    "* Uso di poligrafi (macchine della verità).\n",
    "* Valutare attendibilità delle prove.\n",
    "* Valutare rischio di re-offending (se non solo profiling).\n",
    "* Profilazione durante indagini penali.\n",
    "\n",
    "### g) Migrazione, asilo, frontiere\n",
    "\n",
    "* Poligrafi.\n",
    "* Valutazioni rischi sanitari o irregolarità.\n",
    "* Esame richieste di visto, asilo, permesso di soggiorno.\n",
    "* Identificazione individui, tranne verifica documenti di viaggio.\n",
    "\n",
    "### h) Giustizia e processi democratici\n",
    "\n",
    "* Supporto a giudici e avvocati nell’interpretazione dei fatti e delle leggi.\n",
    "* Sistemi che influenzano direttamente **elezioni o referendum** (escluse IA che solo organizzano campagne senza contatto diretto con l’elettore).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. In sintesi\n",
    "\n",
    "* **Alto rischio** = settori sensibili per sicurezza, diritti, accesso a servizi, lavoro, giustizia.\n",
    "* **Fornitori** → molti obblighi (gestione rischi, dati, documentazione, qualità, supervisione umana).\n",
    "* **Obiettivo** → rendere l’uso dell’IA sicuro, trasparente e controllabile nei contesti più delicati.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee4992-cd95-4d0b-a4ee-03f7d4d65cd4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# AI Act – General Purpose AI (GPAI)\n",
    "\n",
    "## 1. Che cos’è un **modello GPAI**?\n",
    "\n",
    "* **Definizione:** un modello di IA addestrato su **grandi quantità di dati**, spesso con tecniche di **auto-apprendimento (self-supervision)**, che mostra un livello di **generalità**:\n",
    "  → sa svolgere **molti compiti diversi**, non solo uno specifico.\n",
    "* **Caratteristica chiave:** può essere usato direttamente o integrato in **altri sistemi IA** downstream.\n",
    "* **Non rientra**: IA usate solo in fase di ricerca, sviluppo o prototipazione (prima della messa sul mercato).\n",
    "\n",
    " Esempio: GPT-4, Llama 3, Claude, Gemini → sono **modelli GPAI** perché possono generare testo, tradurre, riassumere, scrivere codice, ecc.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Che cos’è un **sistema GPAI**?\n",
    "\n",
    "* Un sistema costruito **a partire da un modello GPAI**.\n",
    "* Può essere:\n",
    "\n",
    "  * **usato direttamente** (es. un chatbot pubblico);\n",
    "  * **integrato in sistemi downstream** (es. un software legale che usa GPT-4 per l’analisi di contratti).\n",
    "\n",
    " Nota: un GPAI system può anche diventare **High Risk**, se usato in un contesto sensibile (es. valutazione studenti, selezione del personale, giustizia).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Obblighi generali per tutti i fornitori di modelli GPAI\n",
    "\n",
    "Tutti i provider devono:\n",
    "\n",
    "1. **Documentazione tecnica**\n",
    "\n",
    "   * Spiegare come è stato addestrato il modello, processi di test, risultati di valutazione.\n",
    "\n",
    "2. **Documentazione per downstream providers**\n",
    "\n",
    "   * Dare istruzioni chiare a chi integra il modello, con info su capacità e limiti → per consentire la conformità di chi sviluppa applicazioni finali.\n",
    "\n",
    "3. **Politica sul diritto d’autore**\n",
    "\n",
    "   * Assicurarsi che l’uso dei dataset rispetti la Direttiva Copyright UE.\n",
    "\n",
    "4. **Trasparenza sui dati di training**\n",
    "\n",
    "   * Pubblicare un **riepilogo sufficientemente dettagliato** dei contenuti usati per addestrare il modello.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Eccezione per i modelli GPAI **open source / open licence**\n",
    "\n",
    "Se il modello è distribuito con **licenza libera e aperta** (parametri, pesi, architettura e uso resi pubblici), i provider devono solo:\n",
    "\n",
    "* rispettare il **copyright**;\n",
    "* pubblicare il **riepilogo sui dati di training**.\n",
    "\n",
    " **Eccezione all’eccezione:** se il modello open source è considerato “**systemic**” (vedi punto 5), valgono **anche gli obblighi aggiuntivi**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. GPAI con **rischio sistemico**\n",
    "\n",
    "### Quando un modello è considerato “systemic”?\n",
    "\n",
    "* Se l’addestramento ha richiesto **> 10^25 FLOPs** (floating point operations).\n",
    "* Obbligo: il provider deve **notificare la Commissione** entro 2 settimane dal rilascio.\n",
    "* Possibile difesa: il provider può provare che, pur superando la soglia, il modello **non presenta rischi sistemici**.\n",
    "* Decisione finale: la Commissione (anche su segnalazione di esperti indipendenti) può dichiarare un modello “**systemic**” se ha **high impact capabilities**.\n",
    "\n",
    "### Obblighi aggiuntivi per i provider di modelli sistemici:\n",
    "\n",
    "1. **Valutazioni e test avversariali** → per identificare e mitigare i rischi.\n",
    "2. **Gestione dei rischi sistemici** → analisi delle fonti, piani di mitigazione.\n",
    "3. **Tracciamento e reporting incidenti** → segnalare immediatamente all’AI Office e alle autorità nazionali.\n",
    "4. **Cybersecurity avanzata** → garantire protezione adeguata contro abusi o attacchi.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Come dimostrare la conformità\n",
    "\n",
    "* Fino alla pubblicazione degli **standard europei armonizzati**, i provider possono aderire **volontariamente a codici di condotta (“codes of practice”)**.\n",
    "* Chi aderisce → ha una **presunzione di conformità**.\n",
    "* Chi non aderisce → deve proporre alla Commissione modalità alternative di rispetto degli obblighi.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Codes of practice (Codici di condotta)\n",
    "\n",
    "### Contenuti\n",
    "\n",
    "* Allineamento con approcci internazionali.\n",
    "* Indicazioni su:\n",
    "\n",
    "  * Cosa inserire nella documentazione tecnica.\n",
    "  * Come descrivere e gestire i rischi sistemici.\n",
    "  * Modalità di risk management lungo tutta la supply chain (anche quando i rischi emergono solo a valle).\n",
    "\n",
    "### Processo di creazione\n",
    "\n",
    "* L’**AI Office** invita i provider GPAI e le autorità nazionali competenti.\n",
    "* Coinvolgimento anche di **società civile, industria, università, sviluppatori downstream, esperti indipendenti**.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Governance e controlli\n",
    "\n",
    "* **AI Office (Commissione UE)**: nuovo organo che vigila sull’attuazione dell’AI Act per i GPAI.\n",
    "* Compiti principali:\n",
    "\n",
    "  * Ricevere **segnalazioni** da provider downstream contro provider upstream.\n",
    "  * Richiedere informazioni ai provider.\n",
    "  * Effettuare **valutazioni dirette dei modelli** quando necessario, specie per indagare rischi sistemici.\n",
    "\n",
    " Se l’AI Office ritiene che un GPAI model non sia conforme → può aprire indagini, imporre correttivi, fino a vietarne l’uso nel mercato UE.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Tempistiche di applicazione (dalla data di entrata in vigore dell’AI Act)\n",
    "\n",
    "* **6 mesi** → entrano in vigore i divieti per sistemi di IA **proibiti**.\n",
    "* **12 mesi** → obblighi per i **GPAI**.\n",
    "* **24 mesi** → obblighi per i sistemi ad alto rischio dell’**Allegato III**.\n",
    "* **36 mesi** → obblighi per i sistemi ad alto rischio dell’**Allegato I**.\n",
    "* **9 mesi** → i **codes of practice** devono essere pronti.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. In sintesi (schema finale)\n",
    "\n",
    "| Categoria            | Obblighi                                                                                        | Esempi                            |\n",
    "| -------------------- | ----------------------------------------------------------------------------------------------- | --------------------------------- |\n",
    "| **Tutti i GPAI**     | Documentazione tecnica + info a downstream + rispetto copyright + riepilogo training data       | GPT-4, Claude, Gemini             |\n",
    "| **Open Source GPAI** | Solo copyright + riepilogo dati (se non sistemici)                                              | Llama 3 open                      |\n",
    "| **Systemic GPAI**    | + valutazioni avversariali + gestione rischi sistemici + report incidenti + cybersecurity       | Modelli >10^25 FLOPs, GPT-4-level |\n",
    "| **Governance**       | AI Office vigila, riceve segnalazioni, può investigare e imporre misure                         | —                                 |\n",
    "| **Tempistiche**      | 6m: divieti; 12m: GPAI; 24m: high risk Annex III; 36m: high risk Annex I; 9m: codici di pratica | —                                 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a9e3d-c0d4-4141-9e2b-20df72a10ecc",
   "metadata": {},
   "source": [
    "# https://artificialintelligenceact.eu/assessment/eu-ai-act-compliance-checker/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5fa477-2566-48db-bdf9-bf2ce5687e79",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Risk Management in ISO/IEC 42001:2023\n",
    "\n",
    "## 1. Processo di base (ciclo di gestione dei rischi)\n",
    "\n",
    "1. **Identificazione e valutazione rischi (Clause 6.1)**\n",
    "\n",
    "   * L’organizzazione analizza i possibili rischi dell’IA (tecnici, etici, legali, sociali).\n",
    "   * Si crea una “mappa” dei rischi più rilevanti per il sistema.\n",
    "\n",
    "2. **Implementazione dei controlli (Clause 8.2)**\n",
    "\n",
    "   * Si applicano **misure operative** per ridurre o mitigare i rischi individuati.\n",
    "   * Esempi: monitoraggio umano, robustezza tecnica, procedure di sicurezza.\n",
    "\n",
    "3. **Monitoraggio e miglioramento (Clauses 9 e 10)**\n",
    "\n",
    "   * I controlli e il sistema IA devono essere:\n",
    "\n",
    "     * **monitorati continuamente**,\n",
    "     * **documentati**,\n",
    "     * **migliorati** con un approccio di ciclo continuo (Plan–Do–Check–Act).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. AIIA (AI Impact Assessment) e DPIA\n",
    "\n",
    "* **AIIA = AI Impact Assessment**\n",
    "\n",
    "  * Simile al DPIA della privacy, ma per l’IA.\n",
    "  * Serve nei casi **ad alto rischio** → guarda agli impatti **sociali, etici e legali**.\n",
    "  * Risponde alla domanda: “Questo sistema IA può generare effetti negativi sulle persone o sulla società?”\n",
    "\n",
    "* **DPIA = Data Protection Impact Assessment**\n",
    "\n",
    "  * Previsto dal GDPR per trattamenti **ad alto rischio sui dati personali**.\n",
    "  * Focalizzato solo su privacy e protezione dati.\n",
    "\n",
    " **Insieme AIIA + DPIA** → forniscono una **visione completa** dei rischi:\n",
    "\n",
    "* **etici/societali** (AIIA)\n",
    "* **legali/di privacy** (DPIA)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Metodologie per condurre AIIA\n",
    "\n",
    "Puoi scegliere gli strumenti/metodologie che meglio si adattano al tuo caso:\n",
    "\n",
    "1. **ISO 31000**\n",
    "\n",
    "   * Standard generale di enterprise risk management (ERM).\n",
    "   * Ti aiuta a integrare i rischi IA nei programmi di gestione rischi aziendali.\n",
    "\n",
    "2. **NIST AI RMF** (AI Risk Management Framework)\n",
    "\n",
    "   * Creato apposta per l’IA.\n",
    "   * Strutturato in 4 funzioni:\n",
    "\n",
    "     * **Map** (mappare i rischi e contesto),\n",
    "     * **Measure** (misurare prestazioni, rischi, bias),\n",
    "     * **Manage** (mitigare e gestire rischi),\n",
    "     * **Govern** (governance e accountability).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Strumenti tecnici per modellare le minacce\n",
    "\n",
    "* **STRIDE**\n",
    "\n",
    "  * Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege.\n",
    "  * Focalizzato su **confidenzialità, integrità, disponibilità**.\n",
    "\n",
    "* **DREAD**\n",
    "\n",
    "  * Damage potential, Reproducibility, Exploitability, Affected users, Discoverability.\n",
    "  * Serve a dare un **punteggio di gravità** alle minacce.\n",
    "\n",
    "* **OWASP per ML**\n",
    "\n",
    "  * Lista delle vulnerabilità tipiche dei sistemi di Machine Learning.\n",
    "  * Utile per valutare rischi di **attacchi adversariali**, fughe di dati, violazioni della privacy.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Obiettivo finale\n",
    "\n",
    "L’adozione di ISO/IEC 42001, con AIIA + DPIA + strumenti di risk modeling, porta a una **IA affidabile (Trustworthy AI)**.\n",
    "Significa:\n",
    "\n",
    "* governance chiara,\n",
    "* metodologie strutturate,\n",
    "* analisi tecnica solida,\n",
    "* riduzione dei rischi lungo tutto il ciclo di vita del sistema IA.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b84917-7b50-44a1-abbd-f69b4e3bb71d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Come sviluppare e controllare un sistema IA conforme a ISO/IEC 42001:2023\n",
    "\n",
    "## 1. **Governance e organizzazione**\n",
    "\n",
    "* **Definisci ruoli e responsabilità**: chi sviluppa, chi approva, chi supervisiona l’IA.\n",
    "* **Stabilisci una policy aziendale sull’IA responsabile** (valori etici, principi di trasparenza, rispetto norme).\n",
    "* **Integra la gestione dei rischi IA** nel sistema di gestione aziendale (ERM, ISO 31000, ISO 27001 se già usi standard di sicurezza).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Identificazione e valutazione rischi (Clause 6.1)**\n",
    "\n",
    "* Conduci una **AI Risk Assessment**: identifica rischi **tecnici, etici, legali e sociali**.\n",
    "* Nei casi ad alto rischio, fai anche una **AI Impact Assessment (AIIA)** → valuta impatti su società, etica, diritti fondamentali.\n",
    "* In parallelo, se tratti dati personali, fai una **DPIA (GDPR)**.\n",
    "\n",
    "**Test utili qui:**\n",
    "\n",
    "* Analisi bias nei dataset.\n",
    "* Simulazioni di casi d’uso improprio.\n",
    "* Threat modeling (STRIDE, DREAD, OWASP-ML).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Controlli e mitigazioni (Clause 8.2)**\n",
    "\n",
    "Implementa misure operative per ridurre i rischi:\n",
    "\n",
    "* **Data governance**: controlla qualità, rappresentatività e correttezza dei dataset (training/validation/test).\n",
    "* **Trasparenza**: documenta capacità, limiti, margini di errore.\n",
    "* **Supervisione umana**: prevedi punti in cui un umano deve validare o poter intervenire.\n",
    "* **Robustezza**: il modello deve resistere a input imprevisti o adversariali.\n",
    "* **Cybersecurity**: proteggi da manipolazioni e accessi non autorizzati.\n",
    "\n",
    "**Test pratici qui:**\n",
    "\n",
    "* Test di accuratezza e generalizzazione.\n",
    "* Test adversariali (input malevoli).\n",
    "* Stress test e fault injection (comportamento sotto carico/condizioni estreme).\n",
    "* Verifica sicurezza dati (privacy by design).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Monitoraggio continuo (Clause 9)**\n",
    "\n",
    "* Monitora prestazioni e rischi durante l’uso reale.\n",
    "* Definisci **KPI** di sicurezza, accuratezza, fairness, explainability.\n",
    "* Attiva un sistema di **incident reporting** per anomalie, violazioni, bias imprevisti.\n",
    "\n",
    "**Test qui:**\n",
    "\n",
    "* Monitoraggio drift dei dati (cambio distribuzione input/output).\n",
    "* Alert su deviazioni dalle performance attese.\n",
    "* Audit periodici (interni o esterni).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Miglioramento continuo (Clause 10)**\n",
    "\n",
    "* Rivedi periodicamente rischi, controlli e procedure.\n",
    "* Aggiorna modelli e dataset con feedback reali.\n",
    "* Documenta ogni modifica e conserva la tracciabilità (audit trail).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Documentazione obbligatoria**\n",
    "\n",
    "Per dimostrare conformità, tieni pronti:\n",
    "\n",
    "* **Fascicolo tecnico**: architettura, dati usati, metriche di valutazione, limiti, controlli di sicurezza.\n",
    "* **Risk register**: elenco rischi identificati e mitigazioni adottate.\n",
    "* **Log ed evidenze**: eventi rilevanti registrati dal sistema.\n",
    "* **Policy e procedure**: supervisione, gestione incidenti, aggiornamenti.\n",
    "\n",
    "---\n",
    "\n",
    "# In sintesi – Controlli/Test da fare\n",
    "\n",
    "1. **Prima del rilascio**:\n",
    "\n",
    "   * Risk Assessment + AIIA/DPIA.\n",
    "   * Bias test sui dataset.\n",
    "   * Accuracy, robustness, fairness test.\n",
    "   * Adversarial e stress test.\n",
    "   * Validazione sicurezza (privacy, accessi, cifratura).\n",
    "\n",
    "2. **Durante l’uso**:\n",
    "\n",
    "   * Monitoraggio continuo (drift, anomalie, errori).\n",
    "   * Incident reporting + root cause analysis.\n",
    "   * Audit periodici.\n",
    "\n",
    "3. **Ciclo di vita**:\n",
    "\n",
    "   * Documentazione sempre aggiornata.\n",
    "   * Revisione periodica di rischi e controlli.\n",
    "   * Miglioramento continuo (Plan–Do–Check–Act).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17475c-4b07-49aa-b0fc-da3100543244",
   "metadata": {},
   "source": [
    "#  Checklist ISO/IEC 42001:2023 – Risk Management per Sistemi di IA\n",
    "\n",
    "## 1. Governance e organizzazione\n",
    "\n",
    "* [ ] Sono stati definiti **ruoli e responsabilità** per lo sviluppo, il controllo e la supervisione dell’IA.\n",
    "* [ ] È stata approvata una **policy aziendale sull’IA responsabile** (valori etici, trasparenza, rispetto leggi).\n",
    "* [ ] La gestione dei rischi IA è integrata nel **sistema di gestione aziendale** (es. ISO 31000, ISO 27001).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Identificazione e valutazione rischi (Clause 6.1)\n",
    "\n",
    "* [ ] È stato condotto un **AI Risk Assessment** (rischi tecnici, etici, legali, sociali).\n",
    "* [ ] Per casi ad alto rischio è stata fatta una **AI Impact Assessment (AIIA)**.\n",
    "* [ ] In caso di trattamento dati personali è stata fatta una **DPIA (GDPR)**.\n",
    "* [ ] Sono stati applicati strumenti di **threat modeling**:\n",
    "\n",
    "  * [ ] STRIDE\n",
    "  * [ ] DREAD\n",
    "  * [ ] OWASP ML\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Controlli e mitigazioni (Clause 8.2)\n",
    "\n",
    "* [ ] Dataset verificati per **qualità, rappresentatività, assenza di errori**.\n",
    "* [ ] Documentate **capacità e limiti del modello** (trasparenza).\n",
    "* [ ] È prevista **supervisione umana** nei punti critici.\n",
    "* [ ] Sono stati implementati **meccanismi di robustezza** contro input imprevisti/adversariali.\n",
    "* [ ] È stata garantita la **cybersecurity** (privacy by design, cifratura, access control).\n",
    "\n",
    "**Test effettuati:**\n",
    "\n",
    "* [ ] Accuracy test (prestazioni su set di validazione/test).\n",
    "* [ ] Robustness test (resilienza ad input fuori distribuzione).\n",
    "* [ ] Fairness test (analisi bias e discriminazioni).\n",
    "* [ ] Adversarial test (attacchi mirati).\n",
    "* [ ] Stress/fault injection test (carico e condizioni estreme).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Monitoraggio continuo (Clause 9)\n",
    "\n",
    "* [ ] Definiti **KPI di sicurezza, accuratezza, fairness, explainability**.\n",
    "* [ ] Attivato un sistema di **incident reporting** e root cause analysis.\n",
    "* [ ] Monitoraggio **drift dei dati** (cambio distribuzione input/output).\n",
    "* [ ] Sono pianificati **audit periodici** (interni o esterni).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Miglioramento continuo (Clause 10)\n",
    "\n",
    "* [ ] Processo di **riesame periodico dei rischi**.\n",
    "* [ ] Processo di **aggiornamento dei modelli/dataset** con feedback reali.\n",
    "* [ ] Tutte le modifiche sono **documentate e tracciabili**.\n",
    "* [ ] Il ciclo di gestione segue il modello **Plan–Do–Check–Act (PDCA)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Documentazione\n",
    "\n",
    "* [ ] Fascicolo tecnico completo (architettura, dataset, metriche, limiti, controlli).\n",
    "* [ ] Registro dei rischi (**risk register**) aggiornato.\n",
    "* [ ] Log ed evidenze di eventi rilevanti conservati.\n",
    "* [ ] Procedure scritte per: supervisione umana, gestione incidenti, aggiornamenti.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3493232-d3d4-4156-b3e1-a2af5e9bbaa5",
   "metadata": {},
   "source": [
    "# https://arxiv.org/abs/2508.08804"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
