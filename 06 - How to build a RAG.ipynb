{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0e9cd2-4915-4a4e-881a-1176aa5338da",
   "metadata": {},
   "source": [
    "### **1. Introduzione al paradigma RAG**\n",
    "\n",
    "#### **1.1. Cos’è un sistema RAG**\n",
    "\n",
    "**Concetto base: Retrieval-Augmented Generation**\n",
    "Il paradigma RAG (Retrieval-Augmented Generation) è una tecnica che unisce modelli di linguaggio generativi (LLM) con meccanismi di recupero di informazioni pertinenti da una base di conoscenza esterna. L’obiettivo è rispondere a domande o generare contenuti basati non solo sulla conoscenza interna del modello, ma anche su dati specifici, aggiornati o personalizzati forniti in tempo reale.\n",
    "\n",
    "A differenza di un modello che genera testi basandosi solo su quanto ha appreso durante il training, RAG consente di ampliare artificialmente la \"memoria\" del modello estraendo contenuti rilevanti da un corpus di documenti, per poi utilizzarli come contesto nella generazione finale.\n",
    "\n",
    "**Architettura ibrida: recupero + generazione**\n",
    "Un sistema RAG è composto da due componenti principali:\n",
    "\n",
    "1. **Retriever**: un modulo che, dato un input (tipicamente una domanda), recupera porzioni di testo rilevanti da una base documentale pre-processata e indicizzata tramite tecniche di similarità semantica. Questo modulo lavora con vector store, embeddings e metadati.\n",
    "\n",
    "2. **Generator**: un modulo basato su LLM che riceve l'input utente più il contesto recuperato dal retriever. A partire da queste informazioni, genera una risposta coerente, contestuale e arricchita.\n",
    "\n",
    "Questa architettura a due fasi consente di superare alcuni limiti tipici degli LLM, come la memoria limitata o la scarsa precisione su dati di nicchia.\n",
    "\n",
    "**Differenza con Prompt-only LLM**\n",
    "Nei modelli di linguaggio tradizionali, l'intero prompt (comprensivo di eventuali documenti, spiegazioni e contesto) deve essere fornito manualmente e in tempo reale. Il modello lavora esclusivamente con le informazioni contenute in quel prompt, senza alcuna capacità di consultare o ricercare conoscenza esterna.\n",
    "\n",
    "Con RAG, invece, il sistema può \"espandere\" automaticamente il prompt, recuperando contenuti pertinenti da una base di dati e fornendoli al modello in modo dinamico. In pratica, il prompt non è più scritto interamente dall'utente, ma è arricchito automaticamente dal sistema in base alla query.\n",
    "\n",
    "**Vantaggi del paradigma RAG rispetto al prompt tradizionale**:\n",
    "\n",
    "* Estensione virtualmente illimitata della conoscenza disponibile.\n",
    "* Possibilità di aggiornare le fonti senza retrain del modello.\n",
    "* Risposte più accurate su domini specifici (ad esempio documentazione tecnica, manuali interni, leggi).\n",
    "* Riduzione di allucinazioni e risposte generiche.\n",
    "\n",
    "Questo approccio è alla base di molte delle applicazioni LLM moderne che richiedono precisione, tracciabilità e aggiornamento continuo delle fonti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f583b9-bbe5-4d81-b32d-acee85171f68",
   "metadata": {},
   "source": [
    "#### **1.2. Perché usare RAG**\n",
    "\n",
    "**Superamento del token limit**\n",
    "I modelli LLM hanno un limite fisico alla quantità di testo che possono gestire in un singolo prompt, noto come *token window*. Ad esempio, GPT-3.5 supporta fino a 16.000 token, mentre GPT-4 arriva a 128.000 token, ma in entrambi i casi c’è un limite rigido. Questo impedisce di includere interi manuali, basi di conoscenza o documentazione estesa direttamente nel prompt.\n",
    "Con RAG, il sistema può memorizzare e indicizzare interi dataset di documenti (anche milioni di righe), estraendo solo le porzioni più rilevanti per ogni domanda. In questo modo, l’input al modello è sempre entro i limiti, ma costruito dinamicamente per ogni richiesta.\n",
    "\n",
    "**Inserimento di conoscenze aggiornate e specifiche**\n",
    "Gli LLM tradizionali non possono conoscere eventi successivi alla loro data di training. Ad esempio, un modello addestrato nel 2023 non sa nulla degli eventi del 2025.\n",
    "Con RAG, è possibile costruire un sistema che interroga documenti aggiornati in tempo reale, come articoli, report aziendali, FAQ, documentazione interna o dati legali aggiornati.\n",
    "Inoltre, è possibile personalizzare la base di conoscenza per ambiti specifici: un RAG per medici userà articoli medici, uno per supporto clienti userà le guide del prodotto, e così via.\n",
    "\n",
    "**Maggiore controllo sulle fonti**\n",
    "Un sistema RAG può essere configurato per restituire, insieme alla risposta, anche i documenti o i paragrafi da cui ha estratto l’informazione. Questo consente di:\n",
    "\n",
    "* Verificare la correttezza della risposta\n",
    "* Tracciare la provenienza dei dati\n",
    "* Gestire la fiducia in ambienti critici (medicina, legale, sicurezza)\n",
    "\n",
    "È anche possibile applicare filtri ai documenti in base ai metadati, in modo da usare solo fonti verificate o aggiornate.\n",
    "\n",
    "**Riduzione di allucinazioni**\n",
    "Uno dei problemi noti degli LLM è l'*hallucination*: il modello genera contenuti apparentemente plausibili ma completamente inventati. Questo è particolarmente rischioso in ambiti sensibili.\n",
    "Con RAG, il modello non deve \"indovinare\" le informazioni, ma le riceve già filtrate e contestuali dal retriever. Questo riduce significativamente la probabilità che vengano fornite risposte false o fuorvianti, specialmente se il prompt include istruzioni come \"basati solo sulle fonti fornite\".\n",
    "\n",
    "In sintesi, l’adozione del paradigma RAG consente di costruire sistemi più **scalabili, affidabili, aggiornabili e verificabili**, andando oltre i limiti strutturali dei modelli linguistici tradizionali.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6259f2a-5948-4583-9dd6-98493525d6ab",
   "metadata": {},
   "source": [
    "#### **1.3. Esempi d’uso reali**\n",
    "\n",
    "**Chatbot su knowledge base aziendale**\n",
    "Uno dei casi d’uso più frequenti per un sistema RAG è la costruzione di un assistente conversazionale che conosce e comprende tutta la documentazione interna di un’azienda: manuali operativi, policy, verbali, documenti di onboarding, domande frequenti.\n",
    "In questo scenario, il chatbot può rispondere a domande come:\n",
    "\n",
    "* “Qual è la procedura per aprire un ticket tecnico?”\n",
    "* “Quali sono le linee guida per il lavoro da remoto?”\n",
    "  Invece di hardcodare le risposte o cercare di inserire tutti i documenti in un prompt unico, il sistema recupera solo i paragrafi rilevanti da un corpus strutturato e li inietta nel prompt.\n",
    "\n",
    "**Assistenti legali o medici**\n",
    "In ambito professionale, un sistema RAG può essere impiegato per l’analisi di documenti giuridici, norme, linee guida cliniche, articoli scientifici o protocolli.\n",
    "Esempi:\n",
    "\n",
    "* Un avvocato può interrogare un assistente RAG per sapere se una clausola è conforme a una normativa vigente, ricevendo anche riferimenti diretti al testo di legge.\n",
    "* Un medico può chiedere raccomandazioni cliniche aggiornate basate su linee guida, articoli accademici o evidenze scientifiche.\n",
    "  Questo approccio è particolarmente utile quando serve **precisione documentale**, e ogni affermazione dev’essere **giustificabile con una fonte**.\n",
    "\n",
    "**Helpdesk e supporto clienti**\n",
    "In aziende con prodotti complessi o molte richieste di assistenza, è possibile costruire un sistema che fornisce risposte immediate e coerenti a domande tecniche o commerciali.\n",
    "Esempi:\n",
    "\n",
    "* “Come resetto la mia password?”\n",
    "* “Quali sono le differenze tra i piani tariffari?”\n",
    "* “Cosa devo fare se il mio dispositivo non si accende?”\n",
    "  In questo caso, il sistema RAG può essere addestrato su FAQ, manuali tecnici, e ticket precedenti per generare risposte contestuali e supportate.\n",
    "\n",
    "**Semantic search avanzata**\n",
    "Oltre ai chatbot, RAG può essere usato per creare motori di ricerca intelligenti che, invece di cercare per parole chiave, **recuperano informazioni per significato**.\n",
    "Ad esempio, in una banca dati di articoli scientifici, si può cercare:\n",
    "\n",
    "* “Quali studi recenti parlano dell’efficacia della terapia X nei pazienti oncologici?”\n",
    "  Il sistema userà l’embedding semantico per trovare i paragrafi più rilevanti, anche se non contengono esattamente quelle parole.\n",
    "  Questo tipo di ricerca è più potente della keyword search classica (come quella di Google) perché tiene conto del **significato** più che della forma.\n",
    "\n",
    "In tutti questi casi, l’approccio RAG rende i sistemi LLM **utilizzabili in contesti professionali**, in cui accuratezza, tracciabilità e aggiornamento delle fonti sono indispensabili.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c1bab-9ae4-45d1-96c6-319925973c9a",
   "metadata": {},
   "source": [
    "### **2. Caricamento dei documenti (Document Loader)**\n",
    "\n",
    "#### **2.1. Cos’è un `Document` in LangChain**\n",
    "\n",
    "**Oggetto `Document`: `page_content + metadata`**\n",
    "In LangChain, ogni unità testuale da usare nel sistema RAG è rappresentata da un oggetto chiamato `Document`.\n",
    "Un `Document` è una semplice struttura dati composta da due elementi principali:\n",
    "\n",
    "* `page_content`: il contenuto testuale vero e proprio, ovvero la parte che sarà usata per generare l'embedding e poi fornita al modello LLM come contesto.\n",
    "* `metadata`: un dizionario opzionale contenente informazioni aggiuntive sulla provenienza del contenuto (es. titolo del file, pagina, sezione, autore, data).\n",
    "\n",
    "Esempio:\n",
    "\n",
    "```python\n",
    "from langchain.schema import Document\n",
    "\n",
    "doc = Document(\n",
    "    page_content=\"Questo è il contenuto di una pagina PDF.\",\n",
    "    metadata={\"source\": \"manuale_tecnico.pdf\", \"page\": 4}\n",
    ")\n",
    "```\n",
    "\n",
    "I metadati **non vengono embeddati**, ma sono molto utili:\n",
    "\n",
    "* per **filtrare** documenti durante la ricerca\n",
    "* per **restituire informazioni di contesto** insieme alla risposta (es. \"Risposta tratta da: manuale\\_tecnico.pdf, pagina 4\")\n",
    "* per effettuare **re-ranking** o aggregazioni\n",
    "\n",
    "**Differenza tra loader grezzo e strutturato**\n",
    "LangChain supporta diversi tipi di document loader, ciascuno progettato per un formato specifico (es. `.txt`, `.pdf`, `.csv`, `.html`, ecc.). Possiamo suddividerli in due categorie:\n",
    "\n",
    "1. **Loader grezzo (raw)**\n",
    "   Questi loader leggono semplicemente il contenuto del file come testo puro, senza interpretare la struttura interna.\n",
    "   Esempi: `TextLoader`, `UnstructuredFileLoader`.\n",
    "\n",
    "   Vantaggi:\n",
    "\n",
    "   * Semplice da usare\n",
    "   * Adatto a testi brevi e lineari\n",
    "\n",
    "   Limiti:\n",
    "\n",
    "   * Perde la struttura semantica (sezioni, titoli, paragrafi)\n",
    "   * Non distingue capitoli, intestazioni, o blocchi\n",
    "\n",
    "2. **Loader strutturato (semantically aware)**\n",
    "   Questi loader, invece, usano modelli o parser avanzati (es. `unstructured`, `pdfminer`, `html.parser`) per ricostruire la struttura del documento e suddividerlo in blocchi logici coerenti.\n",
    "\n",
    "   Esempi:\n",
    "\n",
    "   * `UnstructuredPDFLoader`: estrae testo da PDF con rilevamento di intestazioni, paragrafi, tabelle\n",
    "   * `MarkdownHeaderTextSplitter`: mantiene la gerarchia dei titoli\n",
    "   * `BSHTMLLoader`: pulisce e struttura contenuti HTML\n",
    "\n",
    "   Vantaggi:\n",
    "\n",
    "   * Mantiene la coerenza semantica\n",
    "   * Migliore qualità nei chunking successivi\n",
    "\n",
    "   Limiti:\n",
    "\n",
    "   * Richiede dipendenze esterne\n",
    "   * Più lento e sensibile alla qualità del documento\n",
    "\n",
    "La scelta tra loader grezzo e strutturato dipende dal tipo di fonte.\n",
    "Per esempio, per un dataset `.txt` generato automaticamente è sufficiente un loader semplice, mentre per un manuale tecnico complesso in PDF conviene usare un loader strutturato per mantenere la logica del documento.\n",
    "\n",
    "In sintesi, il caricamento corretto dei documenti rappresenta il primo passo fondamentale per ottenere un RAG efficace, poiché determina la **qualità e la struttura dei dati** che verranno poi spezzettati, indicizzati e utilizzati nelle risposte generate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f072de2-7971-474c-b8a1-20ad63e34802",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> Requisiti:\n",
    "> `pip install -U langchain langchain-community langchain-core langchain-huggingface \\\n",
    "  sentence-transformers faiss-cpu transformers torch accelerate rank-bm25\n",
    "`\n",
    "\n",
    "```python\n",
    "# rag_better_hf.py\n",
    "# ------------------------------------------------------------------------------\n",
    "# SCOPO DEL FILE\n",
    "# ------------------------------------------------------------------------------\n",
    "# Questo script implementa una pipeline RAG (Retrieval-Augmented Generation)\n",
    "# minimale ma robusta usando esclusivamente componenti gratuiti di Hugging Face\n",
    "# e LangChain:\n",
    "#   1) Carica un piccolo corpus (qui simulato come singolo documento).\n",
    "#   2) Lo suddivide in \"chunk\" per migliorare il recupero (retrieval).\n",
    "#   3) Indicizza i chunk in un Vector Store FAISS usando embeddings multilingua.\n",
    "#   4) Crea un retriever ibrido: BM25 (keyword) + vettoriale (semantico).\n",
    "#   5) Costruisce una catena RAG: recupero dei documenti -> iniezione nel prompt\n",
    "#      -> generazione di una risposta con un LLM locale (FLAN-T5).\n",
    "#   6) Pulizia finale dell’output per ridurre ripetizioni.\n",
    "#\n",
    "# NOTE PRATICHE:\n",
    "# - Il modello LLM di default è \"google/flan-t5-base\" (testato su CPU).\n",
    "# - Cambia il modello impostando la variabile d'ambiente HF_LLM (es. flan-t5-small).\n",
    "# - Su Windows, eventuali warning sui symlink HF sono innocui (puoi ignorarli).\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "# Disattiva la telemetria HF (opzionale): riduce rumore in console/log\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
    "\n",
    "from transformers.utils import logging as hf_logging\n",
    "# Silenzia i log dei modelli/transformers (utile in demo/notebook)\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# LangChain: tipi base e utilità per testi\n",
    "# ------------------------------------------------------------------------------\n",
    "from langchain.schema import Document\n",
    "# Text splitter ricorsivo: spezza tenendo conto di strutture gerarchiche (es. titoli)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Nuovi import \"non deprecati\" dal pacchetto langchain-huggingface:\n",
    "# - HuggingFaceEmbeddings: calcolo embeddings tramite modelli sentence-transformers\n",
    "# - HuggingFacePipeline: wrapper di un pipeline HF come LLM per LangChain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "\n",
    "# Vector store FAISS (in memoria): indicizzazione e ricerca per similarità\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Transformers (HF) per il LLM e pipeline di generazione\n",
    "# ------------------------------------------------------------------------------\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Prompt e catene LangChain:\n",
    "# - PromptTemplate: definisce i placeholder {input} e {context}\n",
    "# - create_stuff_documents_chain: concatena i documenti nel prompt (strategia \"stuff\")\n",
    "# - create_retrieval_chain: orchestrazione retrieve -> prompt -> LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Retriever tradizionale (BM25) e Ensemble per combinare BM25 + Vettoriale\n",
    "# ------------------------------------------------------------------------------\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "\n",
    "def build_corpus() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Crea il corpus di partenza. In un caso reale:\n",
    "      - potresti leggere file da disco (PDF, TXT, MD, HTML) via loader dedicati\n",
    "      - arricchire i metadati (es. 'source', 'page', 'section', 'date')\n",
    "    Qui usiamo un testo \"inline\" per massima semplicità.\n",
    "    \"\"\"\n",
    "    file_text = \"\"\"\n",
    "    Product Guide ZX-100\n",
    "\n",
    "    Introduction\n",
    "    The ZX-100 model is a device designed for industrial use.\n",
    "    It supports operating modes A, B, and C. The average power consumption is 45W.\n",
    "\n",
    "    Installation\n",
    "    1) Mount the device on a flat surface.\n",
    "    2) Connect the power supply to 220V.\n",
    "    3) Start in mode A for the initial setup.\n",
    "\n",
    "    Safety\n",
    "    Do not open the panel while the device is powered on.\n",
    "    Maintenance must be performed by qualified personnel.\n",
    "\n",
    "    FAQ\n",
    "    - How do you reset the device? Hold down the RESET button for 10 seconds.\n",
    "    - Difference between mode A and B? Mode B enables detailed logging.\n",
    "    \"\"\"\n",
    "    # Ogni \"Document\" ha: page_content (testo) + metadata (dizionario)\n",
    "    return [Document(page_content=file_text, metadata={\"source\": \"guide_zx100.txt\"})]\n",
    "\n",
    "\n",
    "def make_chunks(documents: List[Document], size: int = 300, overlap: int = 50):\n",
    "    \"\"\"\n",
    "    Suddivide i documenti in chunk di lunghezza 'size' con sovrapposizione 'overlap'.\n",
    "    Perché è importante:\n",
    "      - Gli embeddings lavorano meglio su porzioni concise e coerenti.\n",
    "      - Il retriever recupera passaggi più pertinenti.\n",
    "    'RecursiveCharacterTextSplitter' tende a mantenere coesione semantica meglio\n",
    "    di uno splitter \"cieco\" a caratteri.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=overlap)\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "def make_vectorstore(chunks: List[Document]) -> FAISS:\n",
    "    \"\"\"\n",
    "    Crea il Vector Store (FAISS) a partire dai chunk:\n",
    "      - Calcola embeddings con un modello multilingua (ottimo anche per inglese/italiano).\n",
    "      - Indicizza gli embeddings in FAISS per similarità (cosine di default).\n",
    "    Nota: FAISS qui è in memoria; per persistenza su disco servono metodi di save/load.\n",
    "    \"\"\"\n",
    "    # Modello consigliato per IT/EN: più robusto di MiniLM monolingua su testi misti.\n",
    "    emb = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    )\n",
    "    return FAISS.from_documents(chunks, emb)\n",
    "\n",
    "\n",
    "def build_hf_llm() -> HuggingFacePipeline:\n",
    "    \"\"\"\n",
    "    Costruisce un LLM locale basato su Transformers:\n",
    "      - Default: google/flan-t5-base (sequ2seq, instruction-tuned).\n",
    "      - Per CPU deboli: usa HF_LLM=google/flan-t5-small (più veloce, meno qualità).\n",
    "    Parametri chiave del pipeline:\n",
    "      - max_new_tokens: accorcia le risposte (meno rischio di ripetizioni/deriva).\n",
    "      - do_sample + temperature/top_p/top_k: sampling \"leggero\", riduce loop deterministici.\n",
    "      - repetition_penalty + no_repeat_ngram_size: penalità esplicite contro le ripetizioni.\n",
    "    \"\"\"\n",
    "    model_id = os.getenv(\"HF_LLM\", \"google/flan-t5-base\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    mdl = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "    gen_pipe = pipeline(\n",
    "        task=\"text2text-generation\",\n",
    "        model=mdl,\n",
    "        tokenizer=tok,\n",
    "        max_new_tokens=64,       # risposte corte e mirate al QA\n",
    "        do_sample=True,          # un minimo di stochasticità evita \"eco\"\n",
    "        temperature=0.2,         # bassa creatività (risposte tecniche concise)\n",
    "        top_p=0.9,               # nucleus sampling\n",
    "        top_k=50,                # filtra token a massima probabilità\n",
    "        repetition_penalty=1.15, # scoraggia ripetizioni letterali\n",
    "        no_repeat_ngram_size=3   # evita tri-gram ripetuti\n",
    "    )\n",
    "    # Wrapper LangChain per usare il pipeline HF come LLM in una Chain\n",
    "    return HuggingFacePipeline(pipeline=gen_pipe)\n",
    "\n",
    "\n",
    "# Prompt rigido e minimale: impone lingua, formato, limite alle allucinazioni.\n",
    "# {input} = domanda utente; {context} = concatenazione passaggi recuperati.\n",
    "PROMPT = PromptTemplate.from_template(\n",
    "    \"You are a technical assistant. Answer in ENGLISH using ONLY the context below.\\n\"\n",
    "    \"If the information is not in the context, write exactly: \\\"Not specified in the context.\\\".\\n\"\n",
    "    \"Write 1 clear and concise sentence, no bullet points, do not repeat context text.\\n\\n\"\n",
    "    \"QUESTION:\\n{input}\\n\\n\"\n",
    "    \"CONTEXT:\\n{context}\\n\\n\"\n",
    "    \"ANSWER:\"\n",
    ")\n",
    "\n",
    "\n",
    "def clean_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Post-processing minimale dell'output del LLM:\n",
    "      - rimuove ripetizioni palesi del tipo \"-frase -frase -frase\"\n",
    "      - normalizza spazi multipli\n",
    "      - tronca a 400 caratteri per evitare \"trailing\" indesiderati\n",
    "    Nota: non sostituisce i vincoli del prompt né i parametri anti-ripetizione,\n",
    "    ma aiuta a rifinire ulteriormente le risposte.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"(?:\\s*[-•]\\s*)?(.+?)(?:\\s*(?:-|\\n)\\s*\\1\\b)+\", r\"\\1\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "    return text[:400]\n",
    "\n",
    "\n",
    "def build_hybrid_retriever(chunks: List[Document], db: FAISS):\n",
    "    \"\"\"\n",
    "    Costruisce un retriever ibrido che combina:\n",
    "      - BM25: eccellente per keyword esatte, termini rari, sigle, numeri.\n",
    "      - Vettoriale (FAISS): eccellente per similarità semantica (parafrasi, sinonimi).\n",
    "    La combinazione (EnsembleRetriever) somma i punteggi pesati:\n",
    "      - weights=[0.6, 0.4] -> dà un po' più importanza alla componente semantica.\n",
    "    k (top-k) su entrambi a 4: in totale, il contesto tenderà a includere passaggi\n",
    "    sia \"keyword-match\" che \"semantic-match\".\n",
    "    \"\"\"\n",
    "    bm25 = BM25Retriever.from_documents(chunks)\n",
    "    bm25.k = 4\n",
    "    vec_retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "    return EnsembleRetriever(retrievers=[vec_retriever, bm25], weights=[0.6, 0.4])\n",
    "\n",
    "\n",
    "def answer_with_hf(retriever, user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Pipeline RAG end-to-end per una singola query:\n",
    "      1) Usa il retriever ibrido per ottenere i passaggi più rilevanti.\n",
    "      2) \"Stuff\" chain: concatena i passaggi nel placeholder {context} del prompt.\n",
    "      3) Chiama l'LLM locale (FLAN-T5) per generare la risposta.\n",
    "      4) Applica una pulizia leggera per eliminare eventuali ripetizioni residue.\n",
    "    Ritorna una singola frase, come imposto dal prompt.\n",
    "    \"\"\"\n",
    "    llm = build_hf_llm()\n",
    "    stuff_chain = create_stuff_documents_chain(llm=llm, prompt=PROMPT)\n",
    "    rag_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=stuff_chain)\n",
    "\n",
    "    # IMPORTANTE: le chain \"nuove\" di LangChain si aspettano la chiave 'input'\n",
    "    result = rag_chain.invoke({\"input\": user_query})\n",
    "    return clean_answer(result.get(\"answer\", str(result)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Flusso di esempio:\n",
    "      - costruisce corpus, chunking e vectorstore\n",
    "      - crea retriever ibrido\n",
    "      - esegue alcune query dimostrative\n",
    "    Suggerimenti:\n",
    "      - prova a cambiare HF_LLM e confronta i risultati (small/base)\n",
    "      - prova a cambiare il modello di embeddings (es. all-MiniLM-L6-v2 per EN puro)\n",
    "      - aumenta/diminuisci 'k' e i pesi dell'ensemble per calibrare il retrieval\n",
    "    \"\"\"\n",
    "    docs = build_corpus()\n",
    "    chunks = make_chunks(docs)\n",
    "    db = make_vectorstore(chunks)\n",
    "    retriever = build_hybrid_retriever(chunks, db)\n",
    "\n",
    "    queries = [\n",
    "        \"How do you reset the ZX-100 device?\",\n",
    "        \"What is the difference between mode A and B?\",\n",
    "        \"What is the average power consumption of the device?\",\n",
    "        \"How do you perform the initial installation?\"\n",
    "    ]\n",
    "\n",
    "    for q in queries:\n",
    "        print(\"\\n=== Query (HF local) ===\")\n",
    "        print(q)\n",
    "        print(answer_with_hf(retriever, q))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Note rapide:\n",
    "\n",
    "* Cambia modello LLM con `HF_LLM=google/flan-t5-small python rag_minimal_hf.py` se vuoi più velocità.\n",
    "* Tutto gira su CPU; per GPU basta avere PyTorch con CUDA/ROCm e il pipeline userà la GPU automaticamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c7380-fb19-40b0-9873-74230d54f4de",
   "metadata": {},
   "source": [
    "#### **2.2. Tipologie di loader**\n",
    "\n",
    "In LangChain esistono numerosi **loader specializzati**, progettati per leggere contenuti testuali da diverse fonti e formati. Questi loader convertono i file in oggetti `Document` che possono essere processati nel flusso RAG. La scelta del loader corretto dipende dal tipo di documento e dalla qualità dell’estrazione necessaria.\n",
    "\n",
    "---\n",
    "\n",
    "**TextLoader**\n",
    "Loader semplice e diretto, usato per file `.txt`.\n",
    "Carica l’intero contenuto del file come una singola stringa, senza tentativi di parsing semantico.\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"path/to/file.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "```\n",
    "\n",
    "Vantaggi:\n",
    "\n",
    "* Veloce\n",
    "* Nessuna dipendenza esterna\n",
    "\n",
    "Limiti:\n",
    "\n",
    "* Nessuna struttura semantica\n",
    "* Tutto il contenuto è caricato come un unico blocco\n",
    "\n",
    "---\n",
    "\n",
    "**PyPDFLoader**\n",
    "Loader specifico per file `.pdf`, utilizza `pdfminer.six` per estrarre il testo pagina per pagina.\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"documento.pdf\")\n",
    "documents = loader.load()\n",
    "```\n",
    "\n",
    "Ogni pagina viene caricata come un documento distinto, con metadati relativi alla pagina.\n",
    "Questo è utile quando si vuole effettuare chunking per pagina o identificare facilmente la posizione del contenuto nel file.\n",
    "\n",
    "Vantaggi:\n",
    "\n",
    "* Estrazione pagina per pagina\n",
    "* Facile da usare\n",
    "* Include metadati di pagina\n",
    "\n",
    "Limiti:\n",
    "\n",
    "* Non riconosce titoli o intestazioni\n",
    "* Sensibile alla qualità del PDF (soprattutto se scansioni)\n",
    "\n",
    "---\n",
    "\n",
    "**UnstructuredLoader (via Unstructured.io)**\n",
    "Utilizza una libreria di parsing intelligente per estrarre testo strutturato da PDF, DOCX, HTML, e-mail, ecc.\n",
    "\n",
    "Esempio con PDF:\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "loader = UnstructuredPDFLoader(\"manuale.pdf\")\n",
    "documents = loader.load()\n",
    "```\n",
    "\n",
    "Vantaggi:\n",
    "\n",
    "* Riconosce sezioni, titoli, paragrafi, tabelle\n",
    "* Ideale per documenti complessi o eterogenei\n",
    "* Supporta molti formati\n",
    "\n",
    "Limiti:\n",
    "\n",
    "* Richiede installazione di `unstructured` e librerie di supporto (`pytesseract`, `pdfplumber`, ecc.)\n",
    "* Può essere lento su file lunghi\n",
    "\n",
    "---\n",
    "\n",
    "**DirectoryLoader e glob pattern**\n",
    "Quando si hanno molti file in una cartella (PDF, TXT, ecc.), è possibile usare `DirectoryLoader` per caricarli in batch, specificando un pattern per selezionare solo certi tipi di file.\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path=\"dataset/\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "documents = loader.load()\n",
    "```\n",
    "\n",
    "Opzioni:\n",
    "\n",
    "* `glob=\"**/*.pdf\"`: per includere tutti i PDF, anche in sottocartelle\n",
    "* `loader_cls`: definisce il tipo di loader da usare per ciascun file\n",
    "\n",
    "Vantaggi:\n",
    "\n",
    "* Automazione del caricamento\n",
    "* Adatto a progetti con centinaia di documenti\n",
    "* Compatibile con qualsiasi loader\n",
    "\n",
    "Limiti:\n",
    "\n",
    "* Tutti i file devono essere coerenti con il loader scelto\n",
    "\n",
    "---\n",
    "\n",
    "**Altri loader disponibili in LangChain:**\n",
    "\n",
    "* `CSVLoader`: per file CSV\n",
    "* `WebBaseLoader`: per scaricare e caricare da URL\n",
    "* `BSHTMLLoader`: per pagine HTML usando BeautifulSoup\n",
    "* `NotionDBLoader`, `GoogleDriveLoader`: per fonti cloud o API\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusione:**\n",
    "La scelta del loader è un passaggio cruciale. Se i dati non sono ben estratti o i contenuti non sono divisi logicamente, l’intero sistema RAG ne risentirà. In generale:\n",
    "\n",
    "* Usa `TextLoader` per dati strutturati semplici\n",
    "* Usa `PyPDFLoader` per PDF leggibili pagina per pagina\n",
    "* Usa `UnstructuredLoader` per documenti ricchi e complessi\n",
    "* Usa `DirectoryLoader` per gestire collezioni di documenti in batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719fa500-4f93-41c2-9abb-f5e46a4888c9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2.3 Metadati\n",
    "\n",
    "### Utilità dei metadati (filename, sezione, fonte, ecc.)\n",
    "\n",
    "1. **Filtraggio e selezione avanzata**\n",
    "   I metadati permettono di aggiungere alle porzioni di testo informazioni strutturate, come nome file, titolo sezione, autore, data, numero di pagina, categoria. Questo consente di applicare filtri (ad esempio recuperare solo documenti di una certa data o autore) prima di eseguire la fase di embedding e retrieval.\n",
    "\n",
    "2. **Tracciabilità e contesto delle risposte**\n",
    "   Quando si restituisce una risposta generata, è fondamentale indicare la fonte: sapere da quale file, sezione o pagina proviene l’informazione consente di verificare l’attendibilità e di dare trasparenza all’utente.\n",
    "\n",
    "3. **Ottimizzazione delle prestazioni del retriever**\n",
    "   In scenari con molti documenti, i metadati aiutano a ridurre il corpus iniziale applicando filtri semantici o logici, migliorando la velocità di risposta e la qualità del contesto generato.\n",
    "\n",
    "4. **Ri-ranking e rilevanza contestuale**\n",
    "   I metadati possono essere utilizzati per pesare i risultati durante la fase di retrieval, ad esempio aumentando la priorità di documenti con certe caratteristiche (date più recenti, fonti autorevoli, ecc.).\n",
    "\n",
    "5. **Analisi e monitoraggio del modello**\n",
    "   In fase di sviluppo o produzione, i metadati permettono di conducendo analisi quali: da quali fonti provengono le risposte più efficaci o dove l’LLM genera errori, facilitando debugging e iterazione.\n",
    "\n",
    "### Come personalizzare i metadati durante il caricamento\n",
    "\n",
    "1. **Modifica post‐load dei metadati**\n",
    "   Dopo aver caricato i documenti con un loader, si può iterare sull’elenco restituito e modificare il `metadata` di ciascun `Document`:\n",
    "\n",
    "   ```python\n",
    "   from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "   loader = UnstructuredWordDocumentLoader(\"example.docx\")\n",
    "   documents = loader.load()\n",
    "\n",
    "   for doc in documents:\n",
    "       doc.metadata['source'] = 'my_source_label'\n",
    "       doc.metadata['section'] = 'Introduzione'\n",
    "   ```\n",
    "\n",
    "   In questo modo si aggiungono o sovrascrivono informazioni utili ([GitHub][1], [LangChain][2]).\n",
    "\n",
    "2. **Uso di `metadata_func` in loader JSON o simili**\n",
    "   Alcuni loader, come JSONLoader, supportano la funzione `metadata_func` che permette di estrarre e inserire metadati direttamente dal contenuto dei dati in ingresso:\n",
    "\n",
    "   ```python\n",
    "   from langchain.document_loaders import JSONLoader\n",
    "\n",
    "   def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "       metadata[\"sender_name\"] = record.get(\"sender_name\")\n",
    "       metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")\n",
    "       return metadata\n",
    "\n",
    "   loader = JSONLoader(\n",
    "       file_path=\"chat.json\",\n",
    "       jq_schema=\".messages[]\",\n",
    "       content_key=\"content\",\n",
    "       metadata_func=metadata_func\n",
    "   )\n",
    "   documents = loader.load()\n",
    "   ```\n",
    "\n",
    "   In questo esempio, ogni `Document` includerà i campi `sender_name` e `timestamp_ms` nei metadati ([Medium][3]).\n",
    "\n",
    "3. **Aggiunta automatica di metadati con OpenAI Metadata Tagger**\n",
    "   È possibile automatizzare l’estrazione di metadati strutturati utilizzando strumenti come `OpenAIMetadataTagger`, che impiegano modelli LLM per identificare campi come titolo, tono, data, autore, secondo uno schema JSON definito:\n",
    "\n",
    "   ```python\n",
    "   from langchain_community.document_transformers.openai_functions import create_metadata_tagger\n",
    "   from langchain_openai import ChatOpenAI\n",
    "\n",
    "   schema = {\n",
    "       \"properties\": {\n",
    "           \"movie_title\": {\"type\": \"string\"},\n",
    "           \"critic\": {\"type\": \"string\"},\n",
    "           \"tone\": {\"type\": \"string\", \"enum\": [\"positive\", \"negative\"]},\n",
    "           \"rating\": {\"type\": \"integer\"}\n",
    "       },\n",
    "       \"required\": [\"movie_title\", \"critic\", \"tone\"]\n",
    "   }\n",
    "\n",
    "   llm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0)\n",
    "   transformer = create_metadata_tagger(metadata_schema=schema, llm=llm)\n",
    "   enhanced_docs = transformer.transform_documents(original_documents)\n",
    "   ```\n",
    "\n",
    "   Ogni documento conterrà metadati estratti automaticamente (ad esempio `movie_title`, `critic`, e altri campi), arricchendo le informazioni disponibili per retrieval e filter ([LangChain][4]).\n",
    "\n",
    "4. **Personalizzazione avanzata subclassificando `BaseLoader` o `DirectoryLoader`**\n",
    "   Per esigenze sofisticate, è possibile creare un loader custom estendendo `DirectoryLoader` o `BaseLoader`, ad esempio per includere data di creazione o modifica dei file:\n",
    "\n",
    "   ```python\n",
    "   class DateDirectoryLoader(DirectoryLoader):\n",
    "       def load_file(self, item: Path, path: Path, docs: List[Document], pbar: Optional[Any]) -> None:\n",
    "           super().load_file(item, path, docs, pbar)\n",
    "           if docs:\n",
    "               stat = os.stat(item)\n",
    "               creation = datetime.fromtimestamp(stat.st_ctime).isoformat()\n",
    "               docs[-1].metadata['creation_date'] = creation\n",
    "   ```\n",
    "\n",
    "   Questa tecnica consente di aggiungere metadati di sistema come la data di creazione del file ([Stack Overflow][5]).\n",
    "\n",
    "---\n",
    "\n",
    "### Riepilogo\n",
    "\n",
    "* I metadati offrono **filtraggio, contesto, tracciabilità**, oltre a sostenere ranking e analisi.\n",
    "* Possono essere aggiunti manualmente, tramite funzioni di trasformazione automatizzate, o mediante loader custom.\n",
    "* Integrarli correttamente è fondamentale per costruire un sistema RAG affidabile, trasparente ed efficiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe4ec2-c1bf-4543-8ba6-bae9b824fae6",
   "metadata": {},
   "source": [
    "### **3. Segmentazione dei documenti (Chunking)**\n",
    "\n",
    "#### **3.1. Perché fare chunking**\n",
    "\n",
    "**Limite di token per embedding e prompt**\n",
    "I modelli di embedding e i LLM hanno dei limiti nella quantità di testo che possono gestire in una singola chiamata.\n",
    "Ad esempio:\n",
    "\n",
    "* Il modello `text-embedding-ada-002` ha un limite di 8191 token per input.\n",
    "* I modelli LLM (come GPT-4) hanno anch’essi una finestra massima di contesto (da 8k a 128k token a seconda della versione).\n",
    "\n",
    "Se si tenta di elaborare un documento intero troppo lungo, il sistema restituirà un errore o taglierà il testo, perdendo contenuto utile.\n",
    "Fare chunking consente di **suddividere il documento in parti più piccole**, garantendo che ciascun frammento sia compatibile con i limiti del modello usato, sia per l'embedding sia per l'iniezione nel prompt.\n",
    "\n",
    "---\n",
    "\n",
    "**Miglioramento della granularità semantica**\n",
    "Suddividere un documento in segmenti coerenti aiuta il sistema a recuperare **blocchi significativi**, piuttosto che porzioni casuali di testo.\n",
    "Un buon chunk ha:\n",
    "\n",
    "* una lunghezza equilibrata (né troppo corto né troppo lungo)\n",
    "* una coerenza semantica interna (cioè parla di un solo concetto o paragrafo)\n",
    "\n",
    "Un chunking ben fatto evita problemi come:\n",
    "\n",
    "* perdita di contesto (spezzando frasi o paragrafi a metà)\n",
    "* ambiguità semantica (testo troppo corto o fuori contesto)\n",
    "* scarsa rilevanza nei risultati del retriever\n",
    "\n",
    "Esempio:\n",
    "\n",
    "* Un singolo documento di 10.000 token può essere diviso in 20–30 chunk da 400–600 token, ciascuno rappresentante una sezione autonoma (es. un paragrafo, una voce di FAQ, una pagina).\n",
    "\n",
    "---\n",
    "\n",
    "**Ottimizzazione del recupero**\n",
    "L’efficacia di un sistema RAG dipende in larga parte dalla qualità del *retrieval* (cioè dal processo di recupero dei documenti più pertinenti).\n",
    "Un chunking adeguato:\n",
    "\n",
    "* **aumenta la precisione del retriever**, perché lavora su unità semantiche complete\n",
    "* **migliora la recall**, perché segmenti sovrapposti possono coprire più casi d’uso\n",
    "* **riduce il rumore**: invece di recuperare intere sezioni generiche, otteniamo solo ciò che è utile\n",
    "\n",
    "Inoltre, il chunking consente di:\n",
    "\n",
    "* assegnare **embedding specifici per ogni segmento**, rendendo la rappresentazione semantica più precisa\n",
    "* visualizzare nei risultati di ricerca non l'intero documento, ma esattamente la porzione che ha portato alla risposta\n",
    "\n",
    "---\n",
    "\n",
    "**In sintesi**\n",
    "Il chunking è una fase essenziale e strategica per un sistema RAG robusto, perché:\n",
    "\n",
    "* consente di rientrare nei limiti tecnici dei modelli\n",
    "* migliora la qualità delle risposte\n",
    "* ottimizza le performance del retrieval\n",
    "* permette una gestione più raffinata della conoscenza nei documenti\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b05ef5-9bc3-4224-a6eb-6d6a62b6d7bf",
   "metadata": {},
   "source": [
    "#### **3.2. Strategie di chunking**\n",
    "\n",
    "La segmentazione dei documenti (chunking) può seguire **diverse strategie**, ciascuna adatta a un tipo specifico di contenuto e obiettivo. Scegliere la strategia giusta è essenziale per ottenere un sistema RAG preciso, coerente e performante.\n",
    "\n",
    "---\n",
    "\n",
    "**Fixed-length vs semantico**\n",
    "\n",
    "**Fixed-length (lunghezza fissa)**\n",
    "Questa è la strategia più semplice e usata più frequentemente, soprattutto con tool come LangChain. I documenti vengono divisi in porzioni basate su un numero fisso di caratteri o token.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "* `chunk_size = 500` caratteri\n",
    "* `chunk_overlap = 100` caratteri\n",
    "\n",
    "Il contenuto viene diviso in segmenti di 500 caratteri, ciascuno sovrapposto ai precedenti di 100 caratteri per mantenere continuità tra le sezioni.\n",
    "\n",
    "Vantaggi:\n",
    "\n",
    "* Facile da implementare\n",
    "* Prevedibile e stabile\n",
    "* Utile con modelli che hanno limiti rigidi di token\n",
    "\n",
    "Svantaggi:\n",
    "\n",
    "* Rischio di **tagliare frasi o paragrafi** a metà\n",
    "* Non sempre rispetta la coerenza semantica\n",
    "\n",
    "**Semantico**\n",
    "Questa strategia tenta di suddividere il testo in base a **unità logiche o concettuali**: frasi complete, paragrafi, sezioni, titoli. Si basa su:\n",
    "\n",
    "* punteggiatura (es. fine frase)\n",
    "* marcatori (es. `###` nei Markdown)\n",
    "* struttura HTML (tag come `<h1>`, `<p>`, ecc.)\n",
    "\n",
    "È possibile usarla tramite splitter come:\n",
    "\n",
    "* `MarkdownHeaderTextSplitter`\n",
    "* `SentenceTransformersTextSplitter` (avanzato)\n",
    "\n",
    "Vantaggi:\n",
    "\n",
    "* Chunk coerenti e leggibili\n",
    "* Miglior interpretabilità nella risposta generata\n",
    "* Miglior recupero semantico\n",
    "\n",
    "Svantaggi:\n",
    "\n",
    "* Meno prevedibile in termini di lunghezza\n",
    "* Complessa da implementare con documenti destrutturati\n",
    "\n",
    "---\n",
    "\n",
    "**Chunk overlap: cos’è e perché usarlo**\n",
    "Il *chunk overlap* è il numero di caratteri (o token) condivisi tra un chunk e il successivo. È una tecnica fondamentale per:\n",
    "\n",
    "1. **Evitare perdita di contesto**: se una frase inizia alla fine di un chunk e continua nel successivo, senza overlap si rischia di tagliarla.\n",
    "2. **Mantenere coerenza semantica**: la ripetizione parziale tra blocchi aiuta a non spezzare concetti importanti.\n",
    "3. **Rendere più robusto il retrieval**: una domanda potrebbe essere rilevante per entrambi i chunk sovrapposti.\n",
    "\n",
    "Tipico setup:\n",
    "\n",
    "* `chunk_size = 500`\n",
    "* `chunk_overlap = 100`\n",
    "\n",
    "Quindi ogni nuovo chunk inizia 400 caratteri dopo il precedente (non 500), sovrapponendosi parzialmente.\n",
    "\n",
    "**Trade-off**:\n",
    "\n",
    "* Più overlap = maggiore continuità, ma anche più duplicazione e maggiore uso di memoria\n",
    "* Meno overlap = chunk più distinti, ma rischio di perdita informativa\n",
    "\n",
    "---\n",
    "\n",
    "**Sliding window**\n",
    "Lo sliding window è una strategia specifica di chunking **dinamico e continuo**, dove ogni chunk viene generato spostando una “finestra mobile” sul testo.\n",
    "È un caso particolare della chunking con overlap, ma può essere applicato in modo ancora più flessibile, anche a livello di token o parola.\n",
    "\n",
    "Ad esempio:\n",
    "\n",
    "* Finestra di 50 parole, spostata ogni 10 parole\n",
    "* Ogni finestra è un nuovo chunk\n",
    "\n",
    "Questa tecnica è utile quando:\n",
    "\n",
    "* Si lavora con modelli che richiedono input molto brevi\n",
    "* Si vuole **massimizzare la copertura** e la granularità dell’analisi\n",
    "* Si costruiscono sistemi per **detection locale** (es. analisi di frasi specifiche)\n",
    "\n",
    "Svantaggi:\n",
    "\n",
    "* Maggiore costo computazionale (si generano molti chunk)\n",
    "* Rischio di contenuti ridondanti\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusione**\n",
    "La strategia di chunking va scelta in base al tipo di documento, al modello usato, e all’obiettivo del sistema:\n",
    "\n",
    "| Obiettivo                     | Strategia consigliata      |\n",
    "| ----------------------------- | -------------------------- |\n",
    "| Efficienza e velocità         | Fixed-length + overlap     |\n",
    "| Coerenza semantica            | Chunking semantico         |\n",
    "| Massima copertura             | Sliding window             |\n",
    "| Documenti destrutturati       | Fixed-length conservativo  |\n",
    "| Documenti con gerarchia forte | MarkdownHeaderTextSplitter |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7c407-f0f1-4338-9a01-dab634bda9c8",
   "metadata": {},
   "source": [
    "#### **3.3. Tool per chunking in LangChain**\n",
    "\n",
    "LangChain fornisce una serie di strumenti predefiniti per suddividere documenti in **chunk coerenti** e compatibili con i modelli di embedding e LLM. Ogni splitter ha caratteristiche diverse, pensate per specifici tipi di documento o esigenze di segmentazione.\n",
    "\n",
    "---\n",
    "\n",
    "### **`RecursiveCharacterTextSplitter`**\n",
    "\n",
    "**Descrizione:**\n",
    "È il tool di chunking più versatile e comunemente usato in LangChain. Suddivide il testo usando un approccio ricorsivo, cercando di mantenere porzioni leggibili e complete, partendo dai separatori più \"forti\" (paragrafi, frasi, parole).\n",
    "\n",
    "**Logica di funzionamento:**\n",
    "\n",
    "1. Prova a dividere il testo usando `\\n\\n` (due newline consecutivi).\n",
    "2. Se la lunghezza del chunk è ancora troppo lunga, passa a `\\n` (una sola newline).\n",
    "3. Poi a punteggiatura (`.`, `,`, `;`) e infine spazi.\n",
    "4. Alla fine, se serve, taglia in modo brutale (caratteri puri).\n",
    "\n",
    "**Codice di esempio:**\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "```\n",
    "\n",
    "**Quando usarlo:**\n",
    "\n",
    "* Per la maggior parte dei documenti testuali lineari (.txt, PDF convertiti in testo)\n",
    "* Quando non c’è una struttura gerarchica chiara\n",
    "* Quando si vuole il massimo controllo su lunghezza e continuità\n",
    "\n",
    "---\n",
    "\n",
    "### **`MarkdownHeaderTextSplitter`**\n",
    "\n",
    "**Descrizione:**\n",
    "È uno splitter progettato per documenti in formato Markdown (`.md`), che sfrutta la **gerarchia dei titoli** (es. `#`, `##`, `###`) per suddividere il testo in sezioni logiche e semantiche.\n",
    "\n",
    "**Funzionamento:**\n",
    "\n",
    "* Riconosce i livelli di heading e segmenta il contenuto in base alla struttura gerarchica del documento.\n",
    "* Ogni chunk può includere il contenuto e i titoli di contesto (es. \"Sezione: Sicurezza > Autenticazione\").\n",
    "\n",
    "**Codice di esempio:**\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n",
    "    (\"#\", \"header_1\"),\n",
    "    (\"##\", \"header_2\"),\n",
    "    (\"###\", \"header_3\")\n",
    "])\n",
    "\n",
    "chunks = splitter.split_text(markdown_text)\n",
    "```\n",
    "\n",
    "**Quando usarlo:**\n",
    "\n",
    "* Quando si lavora con documentazione tecnica, note strutturate, guide API, README\n",
    "* Quando si vuole mantenere il **contesto semantico** (es. titolo della sezione da cui proviene il contenuto)\n",
    "* Ideale per prompt con sezioni strutturate o navigazione tra argomenti\n",
    "\n",
    "---\n",
    "\n",
    "### **Custom splitter**\n",
    "\n",
    "**Descrizione:**\n",
    "LangChain consente di creare splitter personalizzati, estendendo la classe base `TextSplitter`. Questo è utile per casi specifici in cui:\n",
    "\n",
    "* la struttura del documento è unica o personalizzata\n",
    "* servono regole aziendali particolari (es. separare per paragrafo + codice)\n",
    "* si vuole applicare pre-processing avanzato prima del chunking\n",
    "\n",
    "**Esempio base di splitter personalizzato:**\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import TextSplitter\n",
    "\n",
    "class CustomSplitter(TextSplitter):\n",
    "    def split_text(self, text: str) -> list[str]:\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        return [p.strip() for p in paragraphs if p.strip()]\n",
    "```\n",
    "\n",
    "**Quando usarlo:**\n",
    "\n",
    "* Quando gli splitter standard non producono chunk coerenti\n",
    "* Se si lavora con log di sistema, output JSON, script tecnici\n",
    "* Se si vogliono regole su misura (es. mantenere codice + spiegazione nel chunk)\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "| Splitter                         | Vantaggi principali                                   | Quando usarlo                        |\n",
    "| -------------------------------- | ----------------------------------------------------- | ------------------------------------ |\n",
    "| `RecursiveCharacterTextSplitter` | Flessibile, controllato, gestisce testo destrutturato | Quasi sempre                         |\n",
    "| `MarkdownHeaderTextSplitter`     | Mantiene contesto semantico, utile con titoli         | Documenti `.md` ben strutturati      |\n",
    "| Custom splitter                  | Adattabile a ogni esigenza                            | Casi aziendali o strutture complesse |\n",
    "\n",
    "Il chunking, se ben progettato, migliora la qualità dell’embedding, l’efficienza del retriever e la coerenza delle risposte del modello. È una fase **non banale** che merita attenzione quanto l'embedding e la generazione.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b870d0-cd04-4d86-aa98-f4a6f8ac8b60",
   "metadata": {},
   "source": [
    "### **4. Generazione degli embedding**\n",
    "\n",
    "#### **4.1. Cos’è un embedding**\n",
    "\n",
    "**Proiezione di un testo in uno spazio vettoriale**\n",
    "Un embedding è una rappresentazione numerica densa di un input testuale (una frase, un paragrafo, una parola), ottenuta attraverso un modello neurale.\n",
    "Lo scopo è quello di **trasformare il testo in un vettore di dimensione fissa**, normalmente composto da centinaia o migliaia di valori reali, che possono essere comparati matematicamente.\n",
    "\n",
    "Questa trasformazione è fondamentale perché:\n",
    "\n",
    "* I modelli e le operazioni computazionali lavorano su numeri, non su parole.\n",
    "* La semantica del testo (cioè il suo significato) viene **\"compressa\" in forma geometrica**.\n",
    "\n",
    "Esempio:\n",
    "Il testo `\"come funziona il motore di ricerca\"` potrebbe essere trasformato in un vettore di 1536 dimensioni, ciascuna con un valore tipo:\n",
    "\n",
    "```\n",
    "[0.023, -0.457, 0.331, ..., -0.112]\n",
    "```\n",
    "\n",
    "Ogni posizione nel vettore cattura una dimensione latente del significato. Più l’embedding è potente, più queste dimensioni risultano informative e precise.\n",
    "\n",
    "---\n",
    "\n",
    "**Vicinanza semantica tra vettori**\n",
    "Il grande vantaggio degli embedding è che **la distanza tra vettori corrisponde a una misura della similarità semantica**.\n",
    "Frasi con significato simile avranno vettori \"vicini\" nello spazio geometrico. Frasi diverse o opposte avranno vettori più lontani.\n",
    "\n",
    "Esempi:\n",
    "\n",
    "* `\"Quanto costa una Tesla?\"` sarà vicino a `\"Prezzo di una Tesla Model Y\"`\n",
    "* `\"Dove si trova Firenze?\"` sarà lontano da `\"Come cucinare il risotto\"`\n",
    "\n",
    "Le misure più comuni per confrontare i vettori sono:\n",
    "\n",
    "* **Cosine similarity**: misura l’angolo tra i vettori (vicini = simili)\n",
    "* **Dot product**: prodotto scalare (più alto = più simile)\n",
    "* **L2 (euclidea)**: distanza geometrica tra due punti\n",
    "\n",
    "Questo comportamento rende gli embedding strumenti perfetti per:\n",
    "\n",
    "* **ricerca semantica** (retrieval)\n",
    "* **clustering** e **classificazione**\n",
    "* **re-ranking** di risposte\n",
    "* **costruzione di sistemi RAG**, dove query e documenti vengono confrontati tramite i loro embedding\n",
    "\n",
    "---\n",
    "\n",
    "**Embedding e RAG**\n",
    "In un sistema RAG, l'embedding è usato sia per:\n",
    "\n",
    "1. **Indicizzare** i chunk dei documenti → ogni chunk è trasformato in un vettore e memorizzato nel vector store.\n",
    "2. **Interpretare** la query dell’utente → anche la domanda viene embeddizzata e confrontata con i documenti indicizzati.\n",
    "\n",
    "Il successo di un sistema RAG dipende in gran parte dalla **qualità di questi vettori**. Se due frasi semanticamente simili finiscono in posizioni molto lontane nello spazio vettoriale, il sistema fallirà nel recuperare i documenti corretti.\n",
    "\n",
    "---\n",
    "\n",
    "In sintesi:\n",
    "\n",
    "* Un embedding è una traduzione matematica del significato di un testo.\n",
    "* La distanza tra embedding corrisponde alla distanza semantica.\n",
    "* Sono alla base del retrieval moderno e dell’intelligenza artificiale linguistica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d05ce73-2225-4b2d-ab11-fd10a85feccb",
   "metadata": {},
   "source": [
    "### **4.2. Modelli disponibili**\n",
    "\n",
    "La qualità degli embedding dipende in larga parte dal **modello utilizzato**. In LangChain e in molti framework di NLP, è possibile scegliere tra modelli **proprietari (API)** e modelli **open-source (locale)**. La scelta dipende da fattori come accuratezza, costo, performance e privacy.\n",
    "\n",
    "---\n",
    "\n",
    "**`text-embedding-ada-002` (OpenAI)**\n",
    "\n",
    "* È il modello di embedding più usato su OpenAI.\n",
    "* Dimensione vettore: 1536\n",
    "* Addestrato su molteplici lingue e tipi di testo.\n",
    "* Ottimo per domande, documenti, codice, query di ricerca, ecc.\n",
    "* Utilizzabile tramite API OpenAI (servizio a pagamento).\n",
    "\n",
    "Esempio di utilizzo in LangChain:\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "```\n",
    "\n",
    "**Pro:**\n",
    "\n",
    "* Alta qualità\n",
    "* Supporto per lingue multiple\n",
    "* Ottima generalizzazione\n",
    "* Manutenzione e aggiornamento garantiti\n",
    "\n",
    "**Contro:**\n",
    "\n",
    "* A pagamento (costo per 1.000 token)\n",
    "* Richiede connessione Internet\n",
    "* Rischi di privacy o regolamentazioni (es. GDPR) se si usa con dati sensibili\n",
    "\n",
    "---\n",
    "\n",
    "**Modelli open-source (locale)**\n",
    "\n",
    "LangChain supporta anche modelli open-source tramite HuggingFace. Tra i più diffusi:\n",
    "\n",
    "* **`all-MiniLM-L6-v2`**\n",
    "\n",
    "  * Velocissimo, dimensione vettore 384\n",
    "  * Ottimo compromesso per uso locale\n",
    "* **`bge-base-en-v1.5`**\n",
    "\n",
    "  * Ottimizzato per ricerca semantica, ottima qualità\n",
    "  * Supporta query/documento ottimizzati\n",
    "* **`Instructor XL`**\n",
    "\n",
    "  * Estremamente potente, accetta istruzioni insieme al testo\n",
    "  * Perfetto per compiti complessi\n",
    "\n",
    "Esempio di utilizzo:\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "```\n",
    "\n",
    "**Pro:**\n",
    "\n",
    "* Gratuito\n",
    "* Nessuna latenza di rete\n",
    "* Possibilità di esecuzione offline\n",
    "* Personalizzabile\n",
    "\n",
    "**Contro:**\n",
    "\n",
    "* Qualità inferiore ai modelli proprietari nei casi complessi\n",
    "* Serve hardware adeguato per i modelli più grandi\n",
    "* Alcuni modelli sono ottimizzati solo per l’inglese\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusione**\n",
    "\n",
    "| Modello                  | Qualità    | Costo       | Lingua         | Uso              |\n",
    "| ------------------------ | ---------- | ----------- | -------------- | ---------------- |\n",
    "| `text-embedding-ada-002` | Alta       | A pagamento | Multilingue    | API OpenAI       |\n",
    "| `MiniLM`                 | Media      | Gratuito    | Multilingue    | Locale           |\n",
    "| `bge-base`               | Alta       | Gratuito    | EN/Multilingue | Locale / ricerca |\n",
    "| `Instructor`             | Molto alta | Gratuito    | EN             | Avanzato NLP     |\n",
    "\n",
    "---\n",
    "\n",
    "### **4.3. Embedding di documenti e query**\n",
    "\n",
    "#### **Embedding di chunk**\n",
    "\n",
    "Ogni documento viene spezzato in *chunk*, e ogni chunk è embeddizzato in fase di indexing.\n",
    "Questo processo permette di costruire un **vector store**, in cui ogni vettore rappresenta un pezzo semantico del corpus.\n",
    "Il chunk può contenere, ad esempio, un paragrafo o una sezione autonoma del documento.\n",
    "È importante che ogni embedding mantenga coerenza e significato isolato.\n",
    "\n",
    "```python\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "```\n",
    "\n",
    "#### **Embedding di query utente**\n",
    "\n",
    "Quando l’utente pone una domanda, anche essa viene trasformata in embedding.\n",
    "Il sistema **non cerca parole chiave**, ma confronta l’embedding della query con quelli dei documenti usando una metrica di similarità.\n",
    "\n",
    "```python\n",
    "query = \"Come si attiva la garanzia del prodotto?\"\n",
    "query_vector = embedding_model.embed_query(query)\n",
    "```\n",
    "\n",
    "Questo consente di recuperare contenuti **semanticamente simili** anche se la formulazione è diversa.\n",
    "\n",
    "#### **Concetto di simmetria semantica**\n",
    "\n",
    "La qualità di un sistema RAG dipende fortemente dal fatto che:\n",
    "\n",
    "* **documenti e query siano rappresentati nello stesso spazio semantico**\n",
    "* i **vettori siano confrontabili** tra loro\n",
    "\n",
    "Questa proprietà è nota come *simmetria semantica*:\n",
    "\n",
    "* `\"Quanto costa un iPhone?\"`\n",
    "* `\"Prezzo medio di un iPhone 14 Pro\"`\n",
    "\n",
    "Devono risultare **vicini** nello spazio degli embedding anche se le parole sono diverse.\n",
    "\n",
    "Per ottenere questa simmetria:\n",
    "\n",
    "* è fondamentale usare un modello addestrato sia per *query* che per *documenti*\n",
    "* alcuni modelli (es. `bge-base`) hanno modalità distinte per query e documenti (query tuning)\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusione**\n",
    "\n",
    "La fase di embedding è il cuore matematico del sistema RAG.\n",
    "Una buona strategia di chunking + scelta accurata del modello di embedding garantiscono:\n",
    "\n",
    "* recupero semantico preciso\n",
    "* risposte pertinenti\n",
    "* alte performance anche su basi di conoscenza estese\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa46d5-ca46-4ae0-b3ac-f8b2df50c5e4",
   "metadata": {},
   "source": [
    "### **5. Vector Store e indicizzazione semantica**\n",
    "\n",
    "#### **5.1. Cos’è un Vector Store**\n",
    "\n",
    "**Archivio vettoriale per embedding testuali**\n",
    "Un **Vector Store** è una struttura dati progettata per **memorizzare, organizzare e interrogare grandi quantità di vettori** ad alta dimensione (gli embedding).\n",
    "Nel contesto di un sistema RAG, ogni *chunk* di documento viene trasformato in un embedding e salvato all’interno del vector store, associato ai suoi metadati.\n",
    "\n",
    "Ogni voce nel vector store contiene:\n",
    "\n",
    "* il vettore numerico (embedding del chunk),\n",
    "* i metadati (es. titolo del documento, numero di pagina, fonte),\n",
    "* il contenuto originale del chunk (testo grezzo).\n",
    "\n",
    "Il vector store è quindi il **cuore dell'indicizzazione semantica**: permette di \"cercare significati\", non parole.\n",
    "\n",
    "Esempio base con FAISS:\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "```\n",
    "\n",
    "In questo esempio:\n",
    "\n",
    "* ogni documento viene trasformato in uno o più vettori,\n",
    "* FAISS li indicizza in modo da poterli confrontare efficientemente in fase di ricerca.\n",
    "\n",
    "---\n",
    "\n",
    "**Interrogazione per similarità**\n",
    "\n",
    "A differenza dei classici database che cercano corrispondenze esatte tra stringhe o valori (es. SQL: `WHERE name = 'Mario'`), un vector store consente di cercare **in base alla vicinanza semantica** tra embedding.\n",
    "\n",
    "Funzionamento:\n",
    "\n",
    "1. L’utente pone una domanda in linguaggio naturale.\n",
    "2. La domanda viene embeddizzata con lo stesso modello usato per i documenti.\n",
    "3. Il sistema cerca i vettori più “vicini” a quello della query.\n",
    "4. I chunk corrispondenti vengono restituiti, con eventuale ordinamento per rilevanza.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever()\n",
    "results = retriever.get_relevant_documents(\"Quali sono le cause dell'inflazione?\")\n",
    "```\n",
    "\n",
    "Il vector store confronta l’embedding della domanda con gli embedding salvati, calcola le distanze (es. cosine similarity), e restituisce i chunk più simili.\n",
    "\n",
    "Questa capacità è ciò che rende un sistema RAG **semanticamente potente**, in grado di:\n",
    "\n",
    "* recuperare informazioni anche con parole diverse da quelle usate nel testo originale,\n",
    "* comprendere domande formulate in modo naturale o non perfetto,\n",
    "* restituire contenuti pertinenti e contestuali.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusione**\n",
    "Un vector store non è un semplice archivio: è uno **spazio semantico interrogabile**, ottimizzato per gestire informazioni complesse in linguaggio naturale.\n",
    "Costruirlo correttamente è essenziale per la riuscita di qualsiasi sistema RAG.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ec9dd-eb34-45a7-a405-17894fff2179",
   "metadata": {},
   "source": [
    "#### **5.2. Opzioni principali**\n",
    "\n",
    "Nel costruire un sistema RAG, la scelta del **vector store** ha un impatto diretto su:\n",
    "\n",
    "* la **scalabilità** del sistema (numero di documenti gestibili),\n",
    "* le **performance di ricerca** (velocità e precisione),\n",
    "* la **gestione della persistenza** dei dati (salvataggio e recupero),\n",
    "* la **modalità di deployment** (locale o cloud),\n",
    "* le **funzionalità avanzate**, come filtri, aggiornamento dinamico, o re-ranking.\n",
    "\n",
    "Di seguito un confronto approfondito delle principali opzioni supportate in LangChain e nel panorama attuale.\n",
    "\n",
    "---\n",
    "\n",
    "### **FAISS – Facebook AI Similarity Search**\n",
    "\n",
    "**Caratteristiche:**\n",
    "\n",
    "* Libreria sviluppata da Meta per la ricerca di similarità su grandi quantità di vettori.\n",
    "* Altamente performante su CPU e GPU.\n",
    "* Lavora interamente **in locale** (nessun server richiesto).\n",
    "\n",
    "**Pro:**\n",
    "\n",
    "* Estremamente veloce, anche con decine di migliaia di vettori.\n",
    "* Supporta varie tecniche di indicizzazione (es. Flat, IVF, HNSW).\n",
    "* Ottimo per prototipi e applicazioni standalone.\n",
    "\n",
    "**Contro:**\n",
    "\n",
    "* **Non ha persistenza nativa**: i dati devono essere salvati e ricaricati manualmente.\n",
    "* Non supporta filtri complessi basati su metadati.\n",
    "* Non è un database, ma una libreria in memoria.\n",
    "\n",
    "**Esempio in LangChain:**\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Qdrant – Vector DB open-source**\n",
    "\n",
    "**Caratteristiche:**\n",
    "\n",
    "* Database vettoriale open-source (Rust + gRPC/HTTP).\n",
    "* Può essere eseguito in locale o su server cloud.\n",
    "* Supporta ricerca per similarità **con metadati e filtri strutturati**.\n",
    "\n",
    "**Pro:**\n",
    "\n",
    "* **Persistente** su disco.\n",
    "* Supporta **filtraggio booleano avanzato** (es. \"source = 'manuale' AND anno > 2022\").\n",
    "* Supporta upsert, delete e aggiornamento dinamico.\n",
    "* Integrazione con HuggingFace Transformers.\n",
    "* Open source, comunità attiva.\n",
    "\n",
    "**Contro:**\n",
    "\n",
    "* Richiede setup con Docker o binario standalone.\n",
    "* Più lento di FAISS per dataset molto piccoli.\n",
    "\n",
    "**Caso d’uso tipico:**\n",
    "\n",
    "* Applicazioni aziendali, progetti self-hosted, sistemi RAG con filtri avanzati.\n",
    "\n",
    "---\n",
    "\n",
    "### **Pinecone – Vector DB as a Service**\n",
    "\n",
    "**Caratteristiche:**\n",
    "\n",
    "* Servizio completamente gestito di vector DB in cloud.\n",
    "* Prestazioni elevate con indicizzazione scalabile (milioni di vettori).\n",
    "* Piattaforma commerciale con vari piani.\n",
    "\n",
    "**Pro:**\n",
    "\n",
    "* **Scalabilità automatica** e gestione del carico.\n",
    "* **Persistenza garantita**, multi-index, backup.\n",
    "* Supporta ricerca ibrida (vector + keyword), e re-ranking.\n",
    "* Interfaccia API semplice.\n",
    "\n",
    "**Contro:**\n",
    "\n",
    "* Richiede account e chiave API.\n",
    "* È un servizio a pagamento (freemium con limiti).\n",
    "* I dati sono esternalizzati (privacy, latenza).\n",
    "\n",
    "**Ideale per:**\n",
    "\n",
    "* Applicazioni in produzione che devono scalare.\n",
    "* Team che non vogliono gestire server o database locali.\n",
    "\n",
    "---\n",
    "\n",
    "### **Altre soluzioni (cenni)**\n",
    "\n",
    "**Weaviate**\n",
    "\n",
    "* Vector DB open-source, con supporto a GraphQL.\n",
    "* Supporta modelli di embedding integrati (\"Bring Your Own Model\").\n",
    "* Persistente, supporta filtri semantici, classi, relazioni tra dati.\n",
    "\n",
    "**Chroma**\n",
    "\n",
    "* Vector DB locale, minimalista, focalizzato su prototipi e applicazioni leggere.\n",
    "* Ottimo in abbinamento con LangChain per piccoli progetti.\n",
    "* Salvataggio automatico in locale (persist directory).\n",
    "\n",
    "**Elasticsearch + KNN plugin**\n",
    "\n",
    "* Motore di ricerca classico che, con plugin, supporta anche ricerca per similarità vettoriale.\n",
    "* Ottimo per sistemi che combinano ricerca full-text e semantica.\n",
    "* Più complesso da configurare.\n",
    "\n",
    "---\n",
    "\n",
    "### **Confronto sintetico**\n",
    "\n",
    "| Sistema  | Tipo      | Persistente  | Filtri avanzati | Deployment     | Performance | Costo       |\n",
    "| -------- | --------- | ------------ | --------------- | -------------- | ----------- | ----------- |\n",
    "| FAISS    | Libreria  | No (manuale) | No              | Locale         | Altissima   | Gratuito    |\n",
    "| Qdrant   | DB        | Sì           | Sì              | Locale / Cloud | Ottima      | Gratuito    |\n",
    "| Pinecone | Cloud     | Sì           | Sì              | Cloud          | Altissima   | A pagamento |\n",
    "| Weaviate | DB        | Sì           | Sì              | Locale / Cloud | Buona       | Gratuito    |\n",
    "| Chroma   | DB locale | Sì           | Limitati        | Locale         | Media       | Gratuito    |\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusione**\n",
    "\n",
    "La scelta del vector store va fatta in base al contesto:\n",
    "\n",
    "* **Sviluppo locale o prototipazione veloce** → FAISS o Chroma\n",
    "* **Sistema self-hosted e con dati aziendali** → Qdrant o Weaviate\n",
    "* **Produzione su larga scala e zero manutenzione** → Pinecone\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73b259-8ea3-4362-aa8c-9505bd00adde",
   "metadata": {},
   "source": [
    "#### **5.3. Costruzione dell’indice**\n",
    "\n",
    "Il processo di **costruzione dell’indice vettoriale** è una fase fondamentale in un sistema RAG. Consiste nel trasformare un corpus testuale in una struttura interrogabile semanticamente, tramite la generazione di embedding e l’inserimento dei vettori in un **vector store**.\n",
    "\n",
    "Vediamo i due aspetti principali:\n",
    "\n",
    "---\n",
    "\n",
    "### **Creazione del database dal corpus**\n",
    "\n",
    "La costruzione dell’indice si articola in questi passaggi fondamentali:\n",
    "\n",
    "1. **Caricamento e segmentazione del corpus**\n",
    "   Si parte da una collezione di documenti caricati e spezzettati in chunk coerenti.\n",
    "\n",
    "   ```python\n",
    "   loader = DirectoryLoader(\"docs/\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "   documents = loader.load()\n",
    "\n",
    "   splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "   chunks = splitter.split_documents(documents)\n",
    "   ```\n",
    "\n",
    "2. **Generazione degli embedding**\n",
    "   Ogni chunk viene convertito in un vettore numerico mediante un modello di embedding.\n",
    "\n",
    "   ```python\n",
    "   from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "   embedding_model = OpenAIEmbeddings()\n",
    "   ```\n",
    "\n",
    "3. **Creazione dell’indice vettoriale**\n",
    "   I chunk embeddizzati vengono salvati in un vector store, come FAISS, Qdrant, Pinecone o altri.\n",
    "\n",
    "   ```python\n",
    "   from langchain.vectorstores import FAISS\n",
    "\n",
    "   vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "   ```\n",
    "\n",
    "Il risultato è un **indice vettoriale semanticamente interrogabile**, dove ogni unità testuale è associata al suo embedding e ai relativi metadati.\n",
    "\n",
    "---\n",
    "\n",
    "### **Salvataggio e ricaricamento**\n",
    "\n",
    "Nel caso di vector store locali (come FAISS o Chroma), è importante **salvare l’indice su disco** per evitare di ricostruirlo ogni volta.\n",
    "\n",
    "#### **FAISS – salvataggio**\n",
    "\n",
    "```python\n",
    "vector_store.save_local(\"indice_faiss\")\n",
    "```\n",
    "\n",
    "#### **FAISS – caricamento**\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.load_local(\"indice_faiss\", embeddings=embedding_model)\n",
    "```\n",
    "\n",
    "Questo approccio è molto utile per:\n",
    "\n",
    "* salvare tempo in ambienti di sviluppo,\n",
    "* riutilizzare l’indice in sessioni successive o in ambienti di produzione,\n",
    "* condividere un indice pre-costruito con altri sistemi.\n",
    "\n",
    "#### **Altri sistemi**\n",
    "\n",
    "* **Qdrant**: è persistente nativamente, non richiede salvataggio manuale.\n",
    "* **Chroma**: può essere inizializzato con un percorso locale persistente:\n",
    "\n",
    "  ```python\n",
    "  Chroma(persist_directory=\"./chroma_db\", ...)\n",
    "  ```\n",
    "* **Pinecone**: essendo un servizio cloud, l’indice è salvato automaticamente lato server.\n",
    "\n",
    "---\n",
    "\n",
    "### **Buone pratiche**\n",
    "\n",
    "* Assicurarsi di usare sempre **lo stesso modello di embedding** per creazione e interrogazione.\n",
    "* Aggiungere **metadati significativi** ai documenti (es. `title`, `source`, `page`, `category`) per facilitare il filtraggio e la tracciabilità.\n",
    "* Se il corpus cambia frequentemente, prevedere **strategie di aggiornamento dinamico** (upsert, delete) o ricostruzione periodica.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusione**\n",
    "La costruzione dell’indice rappresenta il passaggio in cui la conoscenza testuale diventa realmente \"utilizzabile\" dal sistema RAG. Senza un indice ben costruito e persistente, l’intero sistema perde efficienza, coerenza e reattività.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba0ae0-4f70-41ec-9a5c-221097daa821",
   "metadata": {},
   "source": [
    "#### **5.4. Retriever**\n",
    "\n",
    "Il **retriever** è il componente che consente di interrogare un vector store in modo semantico. In un sistema RAG, il retriever è responsabile di prendere in input una **query utente**, convertirla in embedding e restituire i **chunk più simili** dal database vettoriale.\n",
    "\n",
    "Non è solo una semplice funzione di ricerca: può essere **personalizzato** in profondità per controllare la quantità, la qualità e il tipo di contenuti che verranno restituiti al modello generativo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Creazione del retriever**\n",
    "\n",
    "In LangChain, una volta costruito un vector store, è possibile creare un retriever con:\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever()\n",
    "```\n",
    "\n",
    "Questo oggetto implementa il metodo:\n",
    "\n",
    "```python\n",
    "retriever.get_relevant_documents(query: str)\n",
    "```\n",
    "\n",
    "che restituisce una lista di `Document` ordinati per similarità rispetto alla query.\n",
    "\n",
    "Esempio completo:\n",
    "\n",
    "```python\n",
    "query = \"Quali sono le cause dell’inflazione?\"\n",
    "docs_rilevanti = retriever.get_relevant_documents(query)\n",
    "for doc in docs_rilevanti:\n",
    "    print(doc.page_content)\n",
    "```\n",
    "\n",
    "Questa è la base del flusso RAG: il contenuto restituito da `retriever` sarà poi iniettato nel prompt dell’LLM.\n",
    "\n",
    "---\n",
    "\n",
    "### **Personalizzazione**\n",
    "\n",
    "LangChain consente di configurare e raffinare il comportamento del retriever. Le opzioni principali includono:\n",
    "\n",
    "#### **Score Function e Similarity Metric**\n",
    "\n",
    "Il calcolo della similarità tra l’embedding della query e i vettori indicizzati può essere eseguito con:\n",
    "\n",
    "* **cosine similarity** (più usata in NLP)\n",
    "* **dot product**\n",
    "* **L2 (distanza euclidea)**\n",
    "\n",
    "In FAISS, la scelta del metodo dipende dall’indice usato (`IndexFlatIP`, `IndexFlatL2`, ecc.).\n",
    "Nei vector store come Qdrant, la metrica è dichiarata alla creazione della collezione.\n",
    "\n",
    "#### **top\\_k**\n",
    "\n",
    "Indica quanti documenti restituire come rilevanti.\n",
    "Di default `top_k=4`, ma può essere configurato:\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "```\n",
    "\n",
    "Scegliere il giusto `k` è un bilanciamento tra:\n",
    "\n",
    "* **Qualità** (troppi documenti possono “confondere” il modello)\n",
    "* **Copertura** (pochi documenti rischiano di perdere il contenuto corretto)\n",
    "\n",
    "#### **Filtraggio per metadati**\n",
    "\n",
    "Se i documenti contengono metadati (es. `{\"source\": \"manuale.pdf\", \"anno\": 2023}`), è possibile filtrare i risultati solo su subset specifici.\n",
    "\n",
    "Esempio con Qdrant o Pinecone (supportano nativamente i filtri):\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"filter\": {\"source\": \"manuale.pdf\", \"anno\": {\"$gte\": 2022}}\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "Questa funzione è essenziale per:\n",
    "\n",
    "* **limitare la ricerca a documenti rilevanti per il contesto corrente**\n",
    "* **gestire ambienti multi-azienda, multi-prodotto o multilingua**\n",
    "* **eseguire ricerche tematiche o temporali**\n",
    "\n",
    "---\n",
    "\n",
    "### **Esempio pratico completo (FAISS semplificato)**\n",
    "\n",
    "```python\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "query = \"Come resettare il dispositivo?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "for d in docs:\n",
    "    print(f\"{d.metadata['source']}:\\n{d.page_content[:200]}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "Il retriever è il \"motore di ricerca interno\" del sistema RAG.\n",
    "Personalizzarlo consente di:\n",
    "\n",
    "* migliorare l’accuratezza delle risposte\n",
    "* ridurre la quantità di informazioni non pertinenti\n",
    "* ottimizzare le performance del modello generativo\n",
    "\n",
    "Nei sistemi RAG professionali, retriever e vector store sono spesso **ottimizzati e calibrati** tanto quanto il modello LLM stesso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0a7a4-115e-4e5b-8437-4817238de099",
   "metadata": {},
   "source": [
    "### **6. Metriche di similarità**\n",
    "\n",
    "#### **6.1. Definizione**\n",
    "\n",
    "**Cos’è una metrica di similarità: misura della \"vicinanza\" semantica**\n",
    "\n",
    "In un sistema RAG basato su embedding, ogni documento e ogni query vengono rappresentati come **vettori numerici ad alta dimensionalità**. A quel punto, il recupero dei documenti più rilevanti rispetto a una query non avviene più tramite keyword matching, ma confrontando questi vettori.\n",
    "\n",
    "Per confrontare due vettori è necessario **definire una metrica di similarità**, cioè una funzione matematica che quantifica **quanto due vettori sono simili tra loro** in termini geometrici.\n",
    "\n",
    "Nel contesto del NLP, una metrica di similarità:\n",
    "\n",
    "* assume in input due vettori `A` e `B`,\n",
    "* restituisce un valore numerico che rappresenta **quanto sono vicini nel significato**,\n",
    "* è usata per classificare o ordinare i documenti rispetto a una query.\n",
    "\n",
    "La “vicinanza” in questo contesto è **semantica**, non testuale:\n",
    "\n",
    "* `\"Qual è il prezzo di una Tesla?\"`\n",
    "* `\"Quanto costa un'auto elettrica di marca Tesla?\"`\n",
    "  sono frasi testualmente diverse ma semanticamente simili, e i loro vettori avranno una **similarità alta**.\n",
    "\n",
    "Queste metriche sono fondamentali nei **retriever**: servono per confrontare l’embedding della query con tutti gli embedding presenti nell’indice e selezionare i più simili.\n",
    "Un sistema RAG efficace **si basa completamente sulla capacità di queste metriche di catturare la prossimità semantica**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Tipi di metrica (anticipazione del 6.2)\n",
    "\n",
    "La scelta della metrica influisce su:\n",
    "\n",
    "* l’accuratezza del retrieval,\n",
    "* le performance di calcolo,\n",
    "* la compatibilità con il tipo di indice usato (es. FAISS richiede scelte esplicite).\n",
    "\n",
    "Le metriche principali sono:\n",
    "\n",
    "* **Cosine Similarity**\n",
    "* **Dot Product**\n",
    "* **L2 Distance (Euclidean)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3611376-8f24-48de-96b9-00eb24958565",
   "metadata": {},
   "source": [
    "#### **6.2. Tipi di metriche**\n",
    "\n",
    "In un sistema RAG, le metriche di similarità servono per **determinare quali documenti (rappresentati da vettori) sono più vicini alla query (anch’essa rappresentata da un vettore)**. A seconda della metrica scelta, il sistema selezionerà **vettori diversi come “più simili”**, e ciò influenzerà direttamente la qualità del retrieval e delle risposte.\n",
    "\n",
    "Vediamo in dettaglio le tre metriche principali.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cosine Similarity**\n",
    "\n",
    "**Descrizione:**\n",
    "La metrica più usata nei sistemi NLP.\n",
    "Calcola il **coseno dell’angolo** tra due vettori nello spazio, ignorandone la lunghezza. In altre parole, misura **quanto i vettori puntano nella stessa direzione**, indipendentemente dalla loro intensità.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\cdot \\|B\\|}\n",
    "$$\n",
    "\n",
    "dove:\n",
    "\n",
    "* $A \\cdot B$ è il prodotto scalare tra i due vettori,\n",
    "* $\\|A\\|$ e $\\|B\\|$ sono le norme (lunghezze) dei vettori.\n",
    "\n",
    "**Valori:**\n",
    "\n",
    "* 1 → massima similarità (stessa direzione)\n",
    "* 0 → ortogonali (nessuna relazione)\n",
    "* -1 → opposti (molto raro negli embedding NLP)\n",
    "\n",
    "**Vantaggi:**\n",
    "\n",
    "* Robusta in spazi di alta dimensionalità\n",
    "* Invariante rispetto alla scala del vettore (es. normalizza l’intensità)\n",
    "* Riflette bene la similarità semantica tra frasi e documenti\n",
    "\n",
    "**Svantaggi:**\n",
    "\n",
    "* Non tiene conto della lunghezza del contenuto (es. un documento lungo e uno corto ma simili possono essere considerati uguali)\n",
    "\n",
    "**Quando usarla:**\n",
    "\n",
    "* NLP in generale (embedding di frasi, documenti, query)\n",
    "* Sistemi RAG con FAISS, Qdrant, Pinecone, ecc.\n",
    "\n",
    "---\n",
    "\n",
    "### **Dot Product (prodotto scalare)**\n",
    "\n",
    "**Descrizione:**\n",
    "È il semplice **prodotto scalare** tra due vettori, senza normalizzazione.\n",
    "Molto usato internamente nei transformer (es. per l’attenzione) e in modelli addestrati con dot product come metrica nativa.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "A \\cdot B = \\sum_{i=1}^{n} A_i \\cdot B_i\n",
    "$$\n",
    "\n",
    "**Valori:**\n",
    "\n",
    "* Positivo e grande → i vettori puntano nella stessa direzione\n",
    "* Zero → ortogonali\n",
    "* Negativo → direzioni opposte\n",
    "\n",
    "**Vantaggi:**\n",
    "\n",
    "* Più veloce da calcolare (nessuna normalizzazione)\n",
    "* Adatto a modelli che non normalizzano i vettori\n",
    "* È la base del funzionamento di molti modelli neurali\n",
    "\n",
    "**Svantaggi:**\n",
    "\n",
    "* Dipende dalla lunghezza dei vettori (una versione amplificata della cosine similarity)\n",
    "* Non sempre utile se gli embedding non sono stati addestrati con questa metrica in mente\n",
    "\n",
    "**Quando usarla:**\n",
    "\n",
    "* Modelli personalizzati che restituiscono vettori non normalizzati\n",
    "* Retrieval interni a modelli neurali (es. retrieval di token nei transformer)\n",
    "\n",
    "---\n",
    "\n",
    "### **L2 Norm (Euclidean Distance)**\n",
    "\n",
    "**Descrizione:**\n",
    "È la **distanza geometrica classica** tra due punti nello spazio.\n",
    "In pratica, misura “quanto lontani” sono i vettori in termini di coordinate.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "\\text{distance}(A, B) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\n",
    "$$\n",
    "\n",
    "**Valori:**\n",
    "\n",
    "* 0 → i vettori sono identici\n",
    "* Più è grande → meno sono simili\n",
    "\n",
    "**Vantaggi:**\n",
    "\n",
    "* Intuitiva dal punto di vista geometrico\n",
    "* È utile se la distribuzione degli embedding ha significato spaziale diretto\n",
    "\n",
    "**Svantaggi:**\n",
    "\n",
    "* Sensibile alla scala del vettore\n",
    "* Non funziona bene in spazi ad alta dimensionalità (fenomeno della “curse of dimensionality”)\n",
    "* Richiede normalizzazione per essere efficace nel NLP\n",
    "\n",
    "**Quando usarla:**\n",
    "\n",
    "* Sistemi non linguistici (es. computer vision, sensori)\n",
    "* Con modelli che producono embedding omogenei e centrati\n",
    "\n",
    "---\n",
    "\n",
    "### **Confronto sintetico**\n",
    "\n",
    "| Metrica           | Tipo      | Normalizzata | Usata in NLP?  | Performance | Accuratezza semantica |\n",
    "| ----------------- | --------- | ------------ | -------------- | ----------- | --------------------- |\n",
    "| Cosine Similarity | Direzione | Sì           | Sì (molto)     | Alta        | Molto alta            |\n",
    "| Dot Product       | Intensità | No           | Sì (ma meno)   | Altissima   | Dipende dal modello   |\n",
    "| L2 Norm           | Distanza  | No           | No (raramente) | Media       | Bassa/variabile       |\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusione**\n",
    "La metrica **coseno** è quasi sempre la scelta migliore per sistemi NLP e RAG, a meno che:\n",
    "\n",
    "* si utilizzi un modello che specificamente richiede il dot product (es. BERT-as-service),\n",
    "* si lavori in un contesto non linguistico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91177171-fd43-4465-94ef-bacebf36aa10",
   "metadata": {},
   "source": [
    "### **7. Hybrid Search: BM25 + Vector Search**\n",
    "\n",
    "#### **7.1. Cos’è BM25**\n",
    "\n",
    "**Algoritmo classico di ranking (IR tradizionale)**\n",
    "BM25 (Best Matching 25) è uno degli algoritmi più noti e affidabili nel campo del **Information Retrieval tradizionale**.\n",
    "È alla base dei motori di ricerca classici, come Lucene/Elasticsearch, e viene usato per classificare i documenti in base alla loro **rilevanza rispetto a una query di parole chiave**.\n",
    "\n",
    "BM25 fa parte della famiglia degli algoritmi probabilistici e si basa su una logica **statistica**: misura quanto è probabile che un documento sia rilevante per una determinata query, sulla base di quanto spesso compaiono le parole chiave in quel documento rispetto all’intero corpus.\n",
    "\n",
    "---\n",
    "\n",
    "**Calcolo sulla base di keyword matching e frequenza**\n",
    "L’idea centrale di BM25 è che:\n",
    "\n",
    "* **più spesso una parola della query compare in un documento**, più quel documento è rilevante;\n",
    "* ma se la parola è **molto frequente in tutto il corpus**, il suo peso va ridotto (concetto di *IDF*, inverse document frequency);\n",
    "* inoltre, documenti troppo lunghi devono essere penalizzati per non favorire il \"keyword stuffing\".\n",
    "\n",
    "**Formula semplificata:**\n",
    "\n",
    "$$\n",
    "\\text{BM25}(D, Q) = \\sum_{q_i \\in Q} IDF(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})}\n",
    "$$\n",
    "\n",
    "dove:\n",
    "\n",
    "* $q_i$: una parola della query,\n",
    "* $f(q_i, D)$: frequenza di $q_i$ nel documento $D$,\n",
    "* $|D|$: lunghezza del documento,\n",
    "* $\\text{avgdl}$: lunghezza media dei documenti del corpus,\n",
    "* $k_1, b$: parametri di bilanciamento (solitamente $k_1 \\approx 1.2$, $b \\approx 0.75$).\n",
    "\n",
    "---\n",
    "\n",
    "**Caratteristiche di BM25:**\n",
    "\n",
    "* È **lessicale**: lavora a livello di parole, non di significato.\n",
    "* Non ha bisogno di modelli neurali o embedding.\n",
    "* Funziona **molto bene con documenti tecnici o con strutture ripetitive**, dove il vocabolario è limitato e specifico.\n",
    "* È **trasparente e interpretabile**, a differenza delle metriche neurali.\n",
    "\n",
    "---\n",
    "\n",
    "**Limiti di BM25 in un contesto moderno:**\n",
    "\n",
    "* Non comprende sinonimi o riformulazioni (“costo” ≠ “prezzo”).\n",
    "* Non gestisce ambiguità semantica o contesto.\n",
    "* Fallisce quando la query è espressa in modo differente rispetto al testo nel documento.\n",
    "\n",
    "**Esempio:**\n",
    "\n",
    "* Query: `\"Come posso risolvere un errore 404?\"`\n",
    "* Documento: `\"Soluzione per problemi di pagina non trovata\"`\n",
    "  BM25 fallisce, perché non trova corrispondenze lessicali esplicite, anche se il significato è simile.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusione:**\n",
    "BM25 è ancora utile nei contesti dove il **match diretto tra parole è significativo**, e dove si desidera una componente **esatta e interpretabile** nel processo di retrieval.\n",
    "Per questo motivo, è spesso usato **in combinazione con il vector search**, per realizzare un retrieval ibrido che unisca **precisione lessicale** e **comprensione semantica**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da993197-40b5-4753-ad0b-dfbf58a4866f",
   "metadata": {},
   "source": [
    "#### **7.2. Limiti del solo vector search**\n",
    "\n",
    "Il vector search, basato su **embedding semantici**, è estremamente potente nel catturare il significato globale di una frase o di un documento, anche in presenza di sinonimi, riformulazioni o contesto implicito. Tuttavia, questa flessibilità semantica ha anche dei limiti strutturali, soprattutto quando si lavora con contenuti altamente specifici o dove è richiesta precisione lessicale.\n",
    "\n",
    "Ecco i principali limiti del vector search **quando usato da solo**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Fatica con nomi propri, keyword specifiche, date**\n",
    "\n",
    "#### **1. Nomi propri**\n",
    "\n",
    "I modelli di embedding tendono a **generalizzare**, e non sempre riescono a gestire correttamente nomi propri come:\n",
    "\n",
    "* Nomi di persone, luoghi, prodotti o aziende\n",
    "* Titoli di articoli, documenti, libri, sentenze\n",
    "\n",
    "**Esempio:**\n",
    "\n",
    "* Query: “Chi è Mario Rossi?”\n",
    "* Documento: “Mario Rossi è l’autore del manuale XYZ”\n",
    "\n",
    "Se il nome compare una sola volta o in un contesto poco esplicito, il modello potrebbe **non assegnargli sufficiente peso semantico**, e quindi l’embedding della query potrebbe non essere abbastanza vicino a quello del documento.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Keyword tecniche o rare**\n",
    "\n",
    "Il vector search può ignorare **termini rari** o **sigle** molto specifiche, specie se non compaiono spesso nel corpus di training del modello di embedding.\n",
    "\n",
    "**Esempio:**\n",
    "\n",
    "* Query: “Specifiche del protocollo X.509”\n",
    "* Documento: “L’autenticazione si basa sullo standard X.509 definito da…”\n",
    "\n",
    "Se il modello non è stato addestrato su quel dominio, potrebbe non considerare X.509 un concetto significativo.\n",
    "\n",
    "Questo è particolarmente problematico in ambiti:\n",
    "\n",
    "* legali\n",
    "* scientifici\n",
    "* medici\n",
    "* tecnici (manuali, specifiche, standard)\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Date, numeri, versioni**\n",
    "\n",
    "I modelli di embedding **non gestiscono bene numeri precisi, versioni o date**, perché nella rappresentazione vettoriale:\n",
    "\n",
    "* “2021” è trattato come un token privo di contesto temporale preciso,\n",
    "* “versione 1.2.4” potrebbe essere indistinguibile da “1.3” a livello semantico,\n",
    "* le differenze numeriche **non sono lineari né semanticamente rilevanti**.\n",
    "\n",
    "**Esempio:**\n",
    "\n",
    "* Query: “Normativa aggiornata nel 2023”\n",
    "* Documento 1: “Aggiornamento del 2022”\n",
    "* Documento 2: “Revisione del 2023 in vigore da aprile”\n",
    "\n",
    "Il modello potrebbe attribuire **rilevanza simile a entrambi**, anche se la data è un vincolo decisivo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Altri limiti importanti**\n",
    "\n",
    "#### Ambiguità controllata\n",
    "\n",
    "Il vector search tende a **favorire documenti semanticamente vicini**, anche se **non contengono esplicitamente la risposta**. Questo può portare a:\n",
    "\n",
    "* Allucinazioni semantiche (documenti troppo generici ma semanticamente simili)\n",
    "* Rumore nei top-k documenti (chunk irrilevanti ma simili per tono)\n",
    "\n",
    "#### Mancanza di trasparenza\n",
    "\n",
    "Il vector search non restituisce un “perché” un documento è stato scelto.\n",
    "A differenza di BM25, non è immediato dire “questa parola chiave ha avuto peso X”.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "Il vector search eccelle nel trovare **concetti simili**, ma non è affidabile per:\n",
    "\n",
    "* precisione lessicale (nomi propri, sigle, keyword)\n",
    "* dati strutturati (date, versioni, numeri)\n",
    "* domini dove la **presenza esatta di termini** è cruciale\n",
    "\n",
    "Per questo motivo, è buona pratica **combinare il vector search con un sistema di retrieval classico** (es. BM25) attraverso un **approccio ibrido**, che permette di unire il meglio dei due mondi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69a122-cfb8-47cb-9181-c2c89fd2e5c4",
   "metadata": {},
   "source": [
    "#### **7.3. Tecnica ibrida: BM25 + Vector Search**\n",
    "\n",
    "Combining il recupero basato su parole chiave (BM25) con la ricerca semantica (via embeddings) crea un sistema di retrieval più potente e accurato—un approccio ideale per i sistemi RAG più robusti.\n",
    "\n",
    "---\n",
    "\n",
    "### Combinazione ponderata: BM25 + Embeddings\n",
    "\n",
    "L’idea alla base della **hybrid search** è semplice:\n",
    "\n",
    "1. **Lo sparse retriever** (come BM25) eccelle nel trovare parole chiave esatte, nomi propri o dati precisi.\n",
    "2. **Il dense retriever** (basato su embeddings) capisce il significato generale, anche se le parole nella query differiscono da quelle del testo.\n",
    "\n",
    "In un sistema ibrido, entrambi i risultati vengono **uniti e riordinati** secondo una formula ponderata. Ad esempio, si può assegnare un peso 0.4 a BM25 e 0.6 all'embedding-based (FAISS), così da bilanciare precisione lessicale e copertura semantica ([Medium][1], [LangChain][2]).\n",
    "\n",
    "---\n",
    "\n",
    "### Ensemble Retriever: configurazione e tuning\n",
    "\n",
    "LangChain supporta un componente chiamato `EnsembleRetriever` per combinare più retriever in parallelo:\n",
    "\n",
    "* Si definiscono due retriever distinti:\n",
    "\n",
    "  * **BM25Retriever**, per la ricerca lessicale.\n",
    "  * **Vector retriever**, su FAISS o simile, per la similarità semantica.\n",
    "* Poi si crea l’**ensemble retriever** con pesi personalizzabili:\n",
    "\n",
    "```python\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# BM25 retriever su lista di testi (o chunk)\n",
    "bm25 = BM25Retriever.from_documents(documents)\n",
    "bm25.k = 2  # top-k documenti da BM25\n",
    "\n",
    "# Vector retriever su FAISS\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "vector = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Ensemble retriever con pesi (ad esempio 50-50)\n",
    "ensemble = EnsembleRetriever(retrievers=[bm25, vector], weights=[0.5, 0.5])\n",
    "```\n",
    "\n",
    "Il metodo `get_relevant_documents(query)` restituisce i documenti riordinati sulla base del punteggio combinato ([Medium][1], [Superlinked][3]).\n",
    "\n",
    "---\n",
    "\n",
    "### Vantaggi dell’ibrido\n",
    "\n",
    "* Copre **sia la precisione lessicale** (es. nomi propri, date, keyword specifiche) sia la **flessibilità semantica** (sineddoche, sinonimi, contesto implicito).\n",
    "* Risultati più **robusti e rilevanti**, soprattutto su query complesse o su corpus con stile variegato ([Weaviate][4]).\n",
    "* È un modello versatile, facilmente adattabile e calibrabile grazie ai pesi dell’ensemble ([Medium][1], [Superlinked][3]).\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusione\n",
    "\n",
    "L’approccio ibrido consente di superare i limiti intrinseci sia delBM25 (rigidità lessicale) sia del vector search (imprecisione su concetti espliciti). Usando `EnsembleRetriever`, si ottiene un sistema RAG molto più efficace e adatto ai contenuti reali.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82649b-1adf-4105-8c7d-0d80435a95c3",
   "metadata": {},
   "source": [
    "### **8. Conversational Memory e gestione token**\n",
    "\n",
    "#### **8.1. Il problema della memoria nei LLM**\n",
    "\n",
    "I **modelli linguistici di grandi dimensioni (LLM)**, come GPT, Claude o PaLM, sono progettati per generare testo in risposta a un prompt. Tuttavia, questi modelli non possiedono una “memoria a lungo termine” tra le chiamate successive: **ogni interazione avviene in una finestra di contesto limitata**, chiamata **token window**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Token window e dimenticanza**\n",
    "\n",
    "**Cos’è la token window?**\n",
    "È il limite massimo di contenuto che può essere fornito al modello (tra input e output) in una singola interazione. Tutto il testo (prompt, documenti recuperati, cronologia della conversazione, istruzioni) deve stare **entro questo limite**, espresso in **token** (unità minime di testo, simili a parole spezzate).\n",
    "\n",
    "| Modello          | Token Window tipica                        |\n",
    "| ---------------- | ------------------------------------------ |\n",
    "| GPT-3.5-turbo    | 4k – 16k token                             |\n",
    "| GPT-4            | 8k – 128k token (a seconda della versione) |\n",
    "| Claude 2         | fino a 200k token                          |\n",
    "| LLaMA-2 (locale) | \\~4k – 32k token                           |\n",
    "\n",
    "**Cosa succede quando si supera il limite?**\n",
    "\n",
    "* Il contenuto in eccesso viene **tagliato automaticamente**, partendo dalle prime righe del prompt.\n",
    "* Il modello **“dimentica”** le parti rimosse, anche se erano importanti per mantenere il contesto.\n",
    "* Questo può portare a risposte incoerenti, ripetizioni o contraddizioni.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "* Se si fa una domanda riferendosi a una risposta data 10 scambi prima, e il token limit è stato superato, il modello **non sarà più in grado di ricordarla**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Perché è un problema nei sistemi conversazionali (RAG)**\n",
    "\n",
    "I sistemi RAG che usano LLM in modalità conversazionale, come i chatbot aziendali o gli assistenti virtuali, spesso hanno queste caratteristiche:\n",
    "\n",
    "* Lo scambio con l’utente è **progressivo** (domande, follow-up, chiarimenti).\n",
    "* Ogni risposta **dipende dal contesto** (precedenti messaggi o risposte).\n",
    "* Devono **adattarsi dinamicamente** alla conversazione, mantenendo coerenza.\n",
    "\n",
    "Ma con un limite di token:\n",
    "\n",
    "* Dopo una certa lunghezza, **il sistema dimentica le parti iniziali della conversazione**.\n",
    "* Il modello non può “ricordare” messaggi precedenti se non vengono **riportati manualmente nel prompt**, il che però consuma token preziosi.\n",
    "* Inoltre, se il sistema recupera chunk da documenti esterni, questi devono competere con la conversazione nel prompt, aumentando il rischio di dimenticanza.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conseguenze pratiche**\n",
    "\n",
    "* È impossibile mantenere lo storico completo oltre i limiti del modello.\n",
    "* I contenuti recuperati dal retriever potrebbero **non entrare** nel prompt se la memoria conversazionale è troppo lunga.\n",
    "* La **qualità delle risposte decresce** man mano che la finestra viene riempita.\n",
    "* Occorre gestire manualmente **cosa ricordare, cosa riassumere e cosa dimenticare**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "I LLM non hanno una vera “memoria persistente” e lavorano in una finestra limitata.\n",
    "Questo rende necessaria l’implementazione di **strategie di memoria personalizzate** (che vedremo nei punti successivi), per conservare il contesto e mantenere conversazioni coerenti, soprattutto in applicazioni su larga scala o in dialoghi lunghi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185ac34-f57d-4457-a5e0-a7d407d03d84",
   "metadata": {},
   "source": [
    "#### **8.2. Tipi di memoria in LangChain**\n",
    "\n",
    "In LangChain, la **memoria conversazionale** serve per simulare una forma di continuità tra le richieste successive inviate all’LLM. Poiché i modelli non hanno una memoria persistente tra le chiamate, LangChain fornisce componenti chiamati **Memory**, che si occupano di **salvare, aggiornare e fornire il contesto** al modello a ogni interazione.\n",
    "\n",
    "Esistono diversi tipi di memoria, ciascuno con vantaggi e compromessi differenti. Vediamoli uno per uno.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. `ConversationBufferMemory` – cronologia semplice**\n",
    "\n",
    "**Descrizione:**\n",
    "Salva tutto lo storico della conversazione come un **unico blocco testuale** (prompt di tipo: \"User: ...\\nAI: ...\"). Ogni volta che viene fatta una richiesta, l’intero contesto viene concatenato e reinviato al modello.\n",
    "\n",
    "**Caratteristiche principali:**\n",
    "\n",
    "* Salva tutto: nessun riassunto o filtro.\n",
    "* I messaggi vengono ripetuti nel prompt ogni volta.\n",
    "* Perfetta fedeltà della cronologia.\n",
    "\n",
    "**Uso tipico:**\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "```\n",
    "\n",
    "**Vantaggi:**\n",
    "\n",
    "* Facilissimo da usare.\n",
    "* Coerenza perfetta nel breve termine.\n",
    "* Adatto a conversazioni brevi o prototipi.\n",
    "\n",
    "**Svantaggi:**\n",
    "\n",
    "* Quando il numero di token diventa troppo alto, **il prompt esplode** e si taglia il contenuto.\n",
    "* Nessuna strategia di compressione.\n",
    "\n",
    "**Quando usarlo:**\n",
    "\n",
    "* Demo, ambienti controllati, sessioni brevi.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `ConversationSummaryMemory` – riassunto automatico**\n",
    "\n",
    "**Descrizione:**\n",
    "Man mano che la conversazione si allunga e i token crescono, il sistema **riassume automaticamente i messaggi precedenti**, comprimendo il contenuto per rimanere entro il limite della finestra.\n",
    "\n",
    "Utilizza un **modello LLM per generare riassunti semantici** del contesto trascorso.\n",
    "\n",
    "**Uso tipico:**\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=ChatOpenAI())\n",
    "```\n",
    "\n",
    "**Funzionamento:**\n",
    "\n",
    "* Dopo ogni scambio, il sistema aggiorna un riassunto.\n",
    "* Il riassunto viene passato nel prompt come contesto, anziché tutta la cronologia.\n",
    "\n",
    "**Vantaggi:**\n",
    "\n",
    "* Più scalabile: consente di sostenere sessioni lunghe.\n",
    "* Riduce il numero di token consumati.\n",
    "* Mantiene la coerenza delle informazioni centrali.\n",
    "\n",
    "**Svantaggi:**\n",
    "\n",
    "* **Perdita di dettaglio**: alcune informazioni minori possono andare perse nel riassunto.\n",
    "* Richiede un modello per sintetizzare (costo computazionale aggiuntivo).\n",
    "\n",
    "**Quando usarlo:**\n",
    "\n",
    "* Conversazioni estese.\n",
    "* Sistemi di assistenza clienti, coaching, helpdesk.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. `ConversationBufferWindowMemory` – finestra scorrevole**\n",
    "\n",
    "**Descrizione:**\n",
    "Mantiene solo **gli ultimi N scambi** (es. ultimi 3 messaggi utente + 3 risposte AI), ignorando il resto della conversazione.\n",
    "\n",
    "È una via di mezzo tra `BufferMemory` (tutto) e `SummaryMemory` (riassunto): in questo caso si **taglia attivamente la cronologia** più vecchia.\n",
    "\n",
    "**Uso tipico:**\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=3)  # conserva ultimi 3 turni\n",
    "```\n",
    "\n",
    "**Vantaggi:**\n",
    "\n",
    "* Più efficiente in termini di token.\n",
    "* Buona coerenza se l’interazione si basa solo su contenuti recenti.\n",
    "\n",
    "**Svantaggi:**\n",
    "\n",
    "* Nessuna traccia del contesto remoto.\n",
    "* L’utente può confondersi se fa riferimento a messaggi più vecchi.\n",
    "\n",
    "**Quando usarlo:**\n",
    "\n",
    "* Bot informativi con interazioni rapide.\n",
    "* Interfacce in cui si sa che le domande non richiederanno contesto storico.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. `VectorStoreRetrieverMemory` (o `VectorStoreMemory`) – memoria semantica a lungo termine**\n",
    "\n",
    "**Descrizione:**\n",
    "Salva la conversazione in un **vector store**, generando un embedding per ogni messaggio. Quando necessario, viene fatta una ricerca semantica per recuperare i messaggi più rilevanti rispetto alla query corrente.\n",
    "\n",
    "**Uso tipico:**\n",
    "\n",
    "```python\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts([], OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "```\n",
    "\n",
    "**Vantaggi:**\n",
    "\n",
    "* **Memoria persistente e scalabile** anche su centinaia di messaggi.\n",
    "* Recupera contenuti semanticamente vicini, anche se formulati diversamente.\n",
    "* Utile per chatbot su lungo periodo o con interazioni profonde.\n",
    "\n",
    "**Svantaggi:**\n",
    "\n",
    "* Più complesso da configurare.\n",
    "* Il retrieval potrebbe fallire su messaggi brevi o ambigui.\n",
    "* Richiede un buon sistema di embedding.\n",
    "\n",
    "**Quando usarlo:**\n",
    "\n",
    "* Chatbot intelligenti che devono “ricordare” interazioni passate per settimane o mesi.\n",
    "* Sistemi multi-turno in cui la logica della conversazione è distribuita nel tempo.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "Ogni tipo di memoria ha vantaggi specifici. La scelta va fatta in base a:\n",
    "\n",
    "* **Durata e profondità** della conversazione.\n",
    "* **Vincoli di costo** (es. modelli per il riassunto).\n",
    "* **Obiettivo del sistema** (demo, produzione, prototipo, ecc.).\n",
    "\n",
    "| Tipo di memoria      | Caratteristica chiave             | Quando usarla                       |\n",
    "| -------------------- | --------------------------------- | ----------------------------------- |\n",
    "| `BufferMemory`       | Storico completo                  | Conversazioni brevi                 |\n",
    "| `SummaryMemory`      | Riassunto automatico              | Dialoghi lunghi, sistemi reali      |\n",
    "| `BufferWindowMemory` | Solo ultimi N scambi              | Interazioni rapide e recenti        |\n",
    "| `VectorStoreMemory`  | Memoria semantica su vector store | Persistenza e recupero intelligente |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f63756-8e8c-482a-bd2c-4982578c4b90",
   "metadata": {},
   "source": [
    "#### **8.3. Gestione avanzata della memoria conversazionale**\n",
    "\n",
    "Nei sistemi RAG in produzione o nei chatbot con interazioni multi-sessione, non basta tenere in memoria il contesto corrente: è necessario **salvare, recuperare e persistere** lo stato della conversazione, anche dopo ore, giorni o settimane. Questo consente di costruire sistemi più **coerenti, personalizzati e intelligenti**, capaci di \"ricordare\" l’utente.\n",
    "\n",
    "Vediamo come gestire la memoria in modo avanzato in LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "### **Salvataggio e ripristino della conversazione**\n",
    "\n",
    "Le memorie di LangChain possono essere serializzate e deserializzate, così da poter:\n",
    "\n",
    "* salvare lo stato dopo ogni interazione (autosave),\n",
    "* ricaricarlo al login successivo dell’utente,\n",
    "* spostarlo tra ambienti (es. sviluppo → produzione).\n",
    "\n",
    "#### Esempio – `ConversationBufferMemory`\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import json\n",
    "\n",
    "# Creazione della memoria e aggiunta di uno scambio\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Ciao\"}, {\"output\": \"Ciao! Come posso aiutarti?\"})\n",
    "\n",
    "# Serializzazione in JSON\n",
    "json_data = json.dumps(memory.chat_memory.dict())\n",
    "\n",
    "# Salvataggio su file\n",
    "with open(\"memoria.json\", \"w\") as f:\n",
    "    f.write(json_data)\n",
    "\n",
    "# In un secondo momento: ricarico la memoria\n",
    "from langchain.schema import messages_from_dict, messages_to_dict\n",
    "from langchain.memory.chat_memory import ChatMessageHistory\n",
    "\n",
    "with open(\"memoria.json\", \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "# Riconversione e reinserimento nella memoria\n",
    "chat_history = ChatMessageHistory(messages=messages_from_dict(data[\"messages\"]))\n",
    "memory.chat_memory = chat_history\n",
    "```\n",
    "\n",
    "Questo approccio è **compatibile con qualsiasi tipo di memoria**, purché l’oggetto `chat_memory` sia disponibile.\n",
    "\n",
    "---\n",
    "\n",
    "### **Persistenza in JSON, DB o Cloud**\n",
    "\n",
    "#### **1. File JSON (locale)**\n",
    "\n",
    "* Soluzione semplice e leggera.\n",
    "* Ideale per prototipi, piccoli progetti, testing.\n",
    "* Ogni utente può avere un file dedicato (es. `utente_42_memoria.json`).\n",
    "\n",
    "#### **2. SQLite / PostgreSQL / MongoDB**\n",
    "\n",
    "Per progetti reali multiutente, è consigliato salvare la memoria in un database strutturato:\n",
    "\n",
    "* Ogni utente ha una sessione con uno o più messaggi salvati.\n",
    "* È possibile usare ORM (come SQLAlchemy) per mappare il contenuto della conversazione in una tabella.\n",
    "* Ogni messaggio viene salvato come JSON (`{\"role\": \"user\", \"content\": \"...\"}`).\n",
    "\n",
    "Esempio struttura tabella:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE memoria_chat (\n",
    "    utente_id TEXT,\n",
    "    timestamp TIMESTAMP,\n",
    "    messaggi JSONB\n",
    ");\n",
    "```\n",
    "\n",
    "#### **3. Persistenza per vector store memory**\n",
    "\n",
    "Se si utilizza `VectorStoreMemory`, è possibile **salvare il vector store su disco** (FAISS, Chroma) oppure utilizzare una soluzione persistente come Qdrant o Weaviate.\n",
    "\n",
    "```python\n",
    "# Salvataggio FAISS\n",
    "retriever.vectorstore.save_local(\"memoria_semantica\")\n",
    "\n",
    "# Ricarica\n",
    "from langchain.vectorstores import FAISS\n",
    "retriever.vectorstore = FAISS.load_local(\"memoria_semantica\", embedding_model)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Best practice**\n",
    "\n",
    "* Assegna un **ID univoco per ogni utente/sessione**, per distinguere le conversazioni.\n",
    "* Usa **timestamp** o ID incrementale per ordinare cronologicamente i messaggi.\n",
    "* Applica un **limite di profondità o durata** per evitare che le conversazioni crescano all’infinito.\n",
    "* In caso di memory summary, salva anche il **riassunto intermedio**, non solo la cronologia.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "La gestione avanzata della memoria è fondamentale per:\n",
    "\n",
    "* creare chatbot personalizzati,\n",
    "* rendere le risposte più coerenti e contestuali,\n",
    "* garantire un’esperienza continuativa anche tra sessioni diverse.\n",
    "\n",
    "LangChain fornisce gli strumenti per gestire questa persistenza sia in locale (file, vector store) sia su infrastrutture più complesse (DB o cloud).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950ed4b-834b-40d7-8fa5-ba39cf22f756",
   "metadata": {},
   "source": [
    "#### **9.1. Prompt Template dinamico**\n",
    "\n",
    "Nei sistemi RAG, il prompt non è statico. A differenza di un semplice \"chatbot\", qui il prompt viene **costruito dinamicamente a ogni interazione**, combinando:\n",
    "\n",
    "* istruzioni di sistema,\n",
    "* documenti recuperati dal retriever,\n",
    "* messaggi utente,\n",
    "* eventuale memoria conversazionale.\n",
    "\n",
    "Per ottenere risposte coerenti, pertinenti e controllabili, è fondamentale **costruire prompt ben strutturati**, con sezioni chiare e componibili. Questo viene fatto tramite i cosiddetti **prompt template dinamici**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Struttura: istruzione + contesto + domanda**\n",
    "\n",
    "La struttura di un prompt RAG tipico si può rappresentare come:\n",
    "\n",
    "```\n",
    "[System Prompt o istruzione iniziale]\n",
    "+\n",
    "[Contesto recuperato]  ← (chunk dai documenti)\n",
    "+\n",
    "[Domanda utente]\n",
    "```\n",
    "\n",
    "Esempio concreto:\n",
    "\n",
    "```text\n",
    "Sei un assistente esperto in normative fiscali italiane. Rispondi in modo accurato, citando solo i documenti forniti.\n",
    "\n",
    "Contesto:\n",
    "1. \"Nel regime forfettario, l’aliquota IRPEF è ridotta al 5% per i primi cinque anni.\"\n",
    "2. \"L’INPS deve essere calcolata sulla base del reddito imponibile al 25,72%.\"\n",
    "\n",
    "Domanda:\n",
    "Quali sono le aliquote da considerare per un consulente in regime forfettario?\n",
    "```\n",
    "\n",
    "Questo approccio presenta due vantaggi fondamentali:\n",
    "\n",
    "* il **prompt cambia a ogni interazione**, adattandosi ai documenti più rilevanti per quella specifica domanda;\n",
    "* il contesto è **controllato e tracciabile**, riducendo le allucinazioni del modello.\n",
    "\n",
    "In LangChain, si può usare un `PromptTemplate` o `ChatPromptTemplate` per generare dinamicamente il prompt:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Sei un assistente tecnico. Usa esclusivamente i documenti seguenti per rispondere.\n",
    "\n",
    "Contesto:\n",
    "{context}\n",
    "\n",
    "Domanda:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Personalizzazione per tono e stile**\n",
    "\n",
    "Un vantaggio dei LLM è la loro **versatilità stilistica**. Modificando la parte \"istruzione\" del prompt, possiamo guidare la risposta in base al contesto d’uso.\n",
    "\n",
    "#### Esempi di personalizzazione:\n",
    "\n",
    "* **Tono formale e tecnico**:\n",
    "\n",
    "  ```text\n",
    "  Rispondi come un consulente legale esperto, con linguaggio tecnico ma accessibile.\n",
    "  ```\n",
    "\n",
    "* **Tono marketing**:\n",
    "\n",
    "  ```text\n",
    "  Rispondi come un copywriter professionista: tono persuasivo, orientato alla vendita.\n",
    "  ```\n",
    "\n",
    "* **Stile bullet point**:\n",
    "\n",
    "  ```text\n",
    "  Rispondi con un elenco puntato ordinato, evidenziando vantaggi e svantaggi.\n",
    "  ```\n",
    "\n",
    "* **Lingua o dialetto specifico**:\n",
    "\n",
    "  ```text\n",
    "  Rispondi in inglese britannico formale.\n",
    "  Rispondi in italiano semplice, adatto a studenti delle superiori.\n",
    "  ```\n",
    "\n",
    "* **Contesto aziendale specifico**:\n",
    "\n",
    "  ```text\n",
    "  Rispondi come se fossi il chatbot ufficiale di [Nome Azienda], che si rivolge ai clienti con tono cordiale ma professionale.\n",
    "  ```\n",
    "\n",
    "Queste istruzioni possono essere **template-izzate** e inserite all’inizio del prompt, come blocco `system`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Best practice per la costruzione di prompt dinamici RAG**\n",
    "\n",
    "1. **Isola i documenti nel prompt**: ad es. metti ogni chunk numerato o separato da delimitatori (`---`).\n",
    "2. **Includi istruzioni chiare**: specifica che il modello **deve usare solo le fonti fornite**.\n",
    "3. **Aggiungi vincoli di formato**, se desideri output in JSON, elenco puntato, paragrafo breve, ecc.\n",
    "4. **Evita di fornire troppe fonti**: se il contesto è troppo lungo, il modello può confondersi o ignorarne una parte.\n",
    "5. **Sfrutta la funzione `return_source_documents=True`** per esporre all’utente i documenti usati.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "Il prompt dinamico è la vera interfaccia tra retrieval e generazione.\n",
    "Scrivere buoni template significa:\n",
    "\n",
    "* migliorare la qualità della risposta,\n",
    "* ridurre allucinazioni e ambiguità,\n",
    "* adattare lo stile dell’output al proprio brand o dominio.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb4463-58d0-4096-b182-db92cac8fb0b",
   "metadata": {},
   "source": [
    "#### **9.2. LangChain `RetrievalQA` chain**\n",
    "\n",
    "In LangChain, la classe `RetrievalQA` rappresenta una **catena composita** che unisce in modo diretto due elementi fondamentali in un sistema RAG:\n",
    "\n",
    "1. Il **retriever**, responsabile del recupero dei documenti rilevanti dal vector store o da una fonte ibrida (es. ensemble con BM25).\n",
    "2. Il **LLM**, incaricato di generare la risposta a partire dal contesto recuperato.\n",
    "\n",
    "Questa classe fornisce un’interfaccia semplice e modulare per costruire sistemi RAG robusti, componibili e facilmente personalizzabili.\n",
    "\n",
    "---\n",
    "\n",
    "### **Integrazione tra retriever e LLM**\n",
    "\n",
    "Il componente `RetrievalQA` si comporta come una classica LangChain chain: prende un input (in questo caso, la domanda), esegue un recupero dei documenti, costruisce un prompt dinamico con i contenuti trovati, e infine interroga il modello LLM per generare la risposta finale.\n",
    "\n",
    "#### **Esempio base di utilizzo:**\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    retriever=retriever,  # può essere FAISS, Qdrant, ensemble, etc.\n",
    "    return_source_documents=True\n",
    ")\n",
    "```\n",
    "\n",
    "Dopo aver creato la catena, si può eseguire una query con:\n",
    "\n",
    "```python\n",
    "result = qa_chain(\"Quali sono le condizioni per accedere al regime forfettario?\")\n",
    "print(result[\"result\"])  # risposta generata\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Cosa fa internamente:**\n",
    "\n",
    "1. **Embeddizza la query** usando lo stesso modello usato per il corpus.\n",
    "2. **Recupera i chunk più simili** dal vector store tramite il retriever.\n",
    "3. **Inserisce i chunk nel prompt** dinamico usando un template standard (o personalizzato).\n",
    "4. **Chiama l’LLM** con il prompt così costruito.\n",
    "5. **Restituisce la risposta** (e, se richiesto, le fonti).\n",
    "\n",
    "---\n",
    "\n",
    "### **Opzione `return_source_documents`**\n",
    "\n",
    "Passando `return_source_documents=True` durante l’inizializzazione, la catena restituirà anche i **documenti/chunk usati per generare la risposta**.\n",
    "\n",
    "Questo è molto utile in vari casi:\n",
    "\n",
    "* **Debugging**: vedere cosa è stato realmente recuperato dal retriever.\n",
    "* **Auditing e spiegabilità**: mostrare all’utente l’origine delle informazioni.\n",
    "* **Valutazione della qualità del retrieval**: confrontare la risposta con le fonti.\n",
    "\n",
    "#### Esempio:\n",
    "\n",
    "```python\n",
    "result = qa_chain(\"Qual è l'aliquota IRPEF per un forfettario?\")\n",
    "print(\"Risposta:\", result[\"result\"])\n",
    "\n",
    "print(\"\\nFonti:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata.get(\"source\", \"sconosciuto\"))\n",
    "    print(\"  \", doc.page_content[:200], \"...\\n\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Best practice per `RetrievalQA`**\n",
    "\n",
    "* Usa `temperature=0` per risposte più coerenti e meno creative (soprattutto se l’output deve essere preciso e documentato).\n",
    "* Personalizza il `prompt_template` se vuoi controllare il tono o la forma della risposta.\n",
    "* Aggiungi filtri al retriever per usare solo un subset di documenti (es. categoria, lingua, fonte).\n",
    "* Se hai bisogno di maggiore controllo, usa una **custom chain** con `RetrievalQAWithSourcesChain` o `ConversationalRetrievalChain`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "La `RetrievalQA` chain è il punto d’ingresso più semplice e potente per costruire un sistema RAG funzionante con LangChain.\n",
    "Con pochi parametri puoi:\n",
    "\n",
    "* combinare retrieval e generazione,\n",
    "* accedere alle fonti originali,\n",
    "* creare un flusso di risposta dinamico e scalabile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8042c6-caea-4b0b-960b-01c9a966ef56",
   "metadata": {},
   "source": [
    "#### **9.3. Errori comuni nella costruzione del prompt e gestione del contesto**\n",
    "\n",
    "Anche se LangChain automatizza molti aspetti della costruzione di un sistema RAG, esistono **errori frequenti** che degradano la qualità delle risposte generate o rendono il comportamento del sistema incoerente. I due più diffusi riguardano l’uso improprio del **contesto** e la formulazione della **domanda utente**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Contesto troppo lungo**\n",
    "\n",
    "#### Problema:\n",
    "\n",
    "Quando si inserisce nel prompt **un numero eccessivo di chunk** (ad esempio 10–20 blocchi di 500 token), si corre il rischio di:\n",
    "\n",
    "* superare la finestra massima di token del modello (token window),\n",
    "* far “annegare” la domanda in un contesto troppo verboso o ridondante,\n",
    "* introdurre contenuti **non rilevanti** che possono “distrarre” il modello.\n",
    "\n",
    "#### Effetti:\n",
    "\n",
    "* Il modello può ignorare la parte più importante della domanda.\n",
    "* La risposta può essere generica, vaga o fuori tema.\n",
    "* Il comportamento diventa non deterministico: la stessa domanda può produrre risposte diverse, a seconda di quali chunk vengono tagliati.\n",
    "\n",
    "#### Esempio:\n",
    "\n",
    "```text\n",
    "Contesto (10 chunk da 500 token)\n",
    "+\n",
    "Domanda: Qual è l’aliquota IRPEF?\n",
    "\n",
    "→ Il modello risponde con un’introduzione generale alla fiscalità, ma **non fornisce l’aliquota specifica**.\n",
    "```\n",
    "\n",
    "#### Soluzioni:\n",
    "\n",
    "* Imposta un limite ragionato al numero di documenti (`top_k`) restituiti dal retriever (es. 3–5).\n",
    "* Applica **re-ranking** o filtri per selezionare solo i chunk più rilevanti.\n",
    "* Considera strategie come:\n",
    "\n",
    "  * **Chunk scoring**\n",
    "  * **Prompt compressi (raggruppati)**\n",
    "  * **RAG con selezione dinamica per posizione o metadati**\n",
    "\n",
    "---\n",
    "\n",
    "### **Mancanza di struttura nella domanda**\n",
    "\n",
    "#### Problema:\n",
    "\n",
    "Una domanda **troppo vaga o mal strutturata** impedisce al modello di:\n",
    "\n",
    "* capire il contesto richiesto (chi è l’utente? cosa si aspetta?),\n",
    "* scegliere tra ambiguità,\n",
    "* restituire un output formattato correttamente.\n",
    "\n",
    "#### Esempi di domande deboli:\n",
    "\n",
    "* “Mi parli del regime forfettario”\n",
    "* “Quali sono i problemi?”\n",
    "* “Spiegami cosa devo sapere”\n",
    "\n",
    "In queste formulazioni:\n",
    "\n",
    "* non è chiaro cosa si vuole sapere (definizione? vantaggi? requisiti? esempi?).\n",
    "* il modello può **riempire il vuoto con contenuto generico**, anche se il contesto è preciso.\n",
    "\n",
    "#### Effetti:\n",
    "\n",
    "* La risposta è vaga, eccessivamente ampia o parziale.\n",
    "* Il contesto viene sottoutilizzato.\n",
    "* La generazione può introdurre “allucinazioni” non supportate.\n",
    "\n",
    "#### Soluzioni:\n",
    "\n",
    "* **Formulare domande specifiche, chiare, mirate**, ad esempio:\n",
    "\n",
    "  * “Quali sono i requisiti di accesso al regime forfettario secondo la normativa 2023?”\n",
    "  * “Qual è l’aliquota IRPEF applicabile a un consulente nel primo anno?”\n",
    "* Se l’utente finale può inserire la domanda, usare **prompt di sistema che diano esempi di buone domande**.\n",
    "* Integrare sistemi di **rephrasing automatico** o **classificazione dell’intento** per adattare o chiarire la domanda in background.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "Evitare questi errori comuni significa:\n",
    "\n",
    "* ottimizzare l’efficacia del retriever e del modello generativo,\n",
    "* risparmiare token,\n",
    "* aumentare la precisione, la coerenza e la soddisfazione dell’utente.\n",
    "\n",
    "Questi accorgimenti, pur semplici, fanno spesso la differenza tra un RAG mediocre e un sistema realmente utile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19acfb3-f297-4a01-ac74-5e451a71144d",
   "metadata": {},
   "source": [
    "#### **10.1. Precision\\@k**\n",
    "\n",
    "Quando si costruisce un sistema RAG, è fondamentale **valutare le sue prestazioni in modo oggettivo**, sia per migliorarlo iterativamente che per confrontare alternative (modelli, retriever, strategie di chunking, etc.).\n",
    "Una delle metriche fondamentali per valutare **la qualità del recupero dei documenti** è la **precision\\@k**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Definizione: % di documenti rilevanti nei primi k**\n",
    "\n",
    "**Precision\\@k** (precision at top-k) misura **quanti dei primi `k` documenti restituiti dal retriever sono effettivamente rilevanti** per una determinata domanda.\n",
    "\n",
    "$$\n",
    "\\text{Precision@k} = \\frac{\\text{Numero di documenti rilevanti nei primi k}}{k}\n",
    "$$\n",
    "\n",
    "#### Esempio:\n",
    "\n",
    "Supponiamo che il sistema restituisca i primi 5 documenti (`k = 5`), e che 3 di questi contengano effettivamente l'informazione necessaria per rispondere correttamente alla domanda.\n",
    "Allora:\n",
    "\n",
    "$$\n",
    "\\text{Precision@5} = \\frac{3}{5} = 0.6\n",
    "$$\n",
    "\n",
    "**Interpretazione:**\n",
    "\n",
    "* Precision\\@k = 1 → tutti i documenti recuperati sono rilevanti\n",
    "* Precision\\@k = 0 → nessun documento utile è stato trovato tra i primi k\n",
    "\n",
    "**Nota importante:**\n",
    "La precision\\@k **valuta solo il retriever**, non il modello generativo. È utile per sapere se il sistema ha *recuperato* i dati giusti prima ancora di costruire il prompt per l'LLM.\n",
    "\n",
    "---\n",
    "\n",
    "### **Calcolo manuale vs automatico**\n",
    "\n",
    "#### **Calcolo manuale**\n",
    "\n",
    "Nel calcolo manuale, un valutatore umano (o docente, nel caso di un progetto didattico) esamina i documenti recuperati rispetto a una domanda e decide quali sono:\n",
    "\n",
    "* **rilevanti** (contengono la risposta in modo esplicito, corretto e utile),\n",
    "* **non rilevanti** (fuori tema, vaghi o fuorvianti).\n",
    "\n",
    "**Esempio di checklist manuale:**\n",
    "\n",
    "```text\n",
    "Domanda: \"Qual è l’aliquota IRPEF nel regime forfettario?\"\n",
    "Documenti restituiti:\n",
    "1. Sì – contiene la risposta corretta → ✔\n",
    "2. No – parla della gestione INPS → ✘\n",
    "3. Sì – riporta aliquota aggiornata → ✔\n",
    "4. Sì – contiene confronto tra aliquote → ✔\n",
    "5. No – parla del regime ordinario → ✘\n",
    "\n",
    "Precision@5 = 3/5 = 0.6\n",
    "```\n",
    "\n",
    "Questa modalità è ideale per:\n",
    "\n",
    "* progetti pilota,\n",
    "* testing qualitativo,\n",
    "* dataset piccoli con supervisione.\n",
    "\n",
    "#### **Calcolo automatico**\n",
    "\n",
    "In ambienti più grandi o automatizzati, è possibile calcolare la precision\\@k in modo **semi-automatico o automatico**:\n",
    "\n",
    "1. **Ground truth**: si predispone un dataset di domande con le fonti corrette (documenti rilevanti per ciascuna).\n",
    "2. Si esegue il retrieval per ogni domanda.\n",
    "3. Si confrontano i documenti restituiti con quelli attesi.\n",
    "\n",
    "Questo è particolarmente utile in pipeline con:\n",
    "\n",
    "* benchmark automatizzati,\n",
    "* test A/B tra retriever diversi,\n",
    "* ottimizzazione iterativa (ad esempio con `Optuna` o altri tool di tuning).\n",
    "\n",
    "**Attenzione:**\n",
    "Il calcolo automatico della precision richiede che i documenti abbiano **ID stabili** (es. filename + numero di pagina o hash) per poter confrontare in modo affidabile le corrispondenze tra retrieved e expected.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "**Precision\\@k** è una metrica semplice ma potente per:\n",
    "\n",
    "* diagnosticare errori del retriever,\n",
    "* confrontare configurazioni diverse (modelli, chunking, metadati),\n",
    "* valutare quanto il sistema recupera **effettivamente** informazioni utili.\n",
    "\n",
    "Nella pratica, va usata **in combinazione con metriche generative**, come la qualità della risposta finale (vedi 10.2), per ottenere un quadro completo delle performance del sistema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb78e4b3-dc44-4145-a0e4-123896ca19d7",
   "metadata": {},
   "source": [
    "#### **10.2. Valutazione della risposta**\n",
    "\n",
    "Dopo aver valutato la qualità del **retrieval** (ad es. con precision\\@k), è fondamentale misurare la **correttezza e utilità della risposta generata dal LLM**.\n",
    "Questa fase di valutazione mira a rispondere a domande come:\n",
    "\n",
    "* La risposta è **corretta** rispetto al contesto fornito?\n",
    "* È **completa**, **coerente**, **non allucinata**?\n",
    "* Riflette fedelmente il contenuto dei documenti recuperati?\n",
    "\n",
    "Esistono due approcci principali:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Valutazione Manuale (Human Evaluation)**\n",
    "\n",
    "Il metodo più affidabile ma costoso in termini di tempo. Un umano valuta se la risposta è:\n",
    "\n",
    "| Criterio             | Descrizione                                                         |\n",
    "| -------------------- | ------------------------------------------------------------------- |\n",
    "| **Correctness**      | La risposta è corretta rispetto alla domanda?                       |\n",
    "| **Faithfulness**     | L’output si basa solo sul contesto fornito (no allucinazioni)?      |\n",
    "| **Completeness**     | La risposta copre tutti gli aspetti rilevanti della domanda?        |\n",
    "| **Readability**      | È comprensibile, ben scritta, coerente?                             |\n",
    "| **Source alignment** | Le affermazioni fatte nella risposta sono supportate dai documenti? |\n",
    "\n",
    "#### Esempio di scala a 3 punti:\n",
    "\n",
    "* **2 = Corretta e supportata**\n",
    "* **1 = Parzialmente corretta o incompleta**\n",
    "* **0 = Sbagliata o allucinata**\n",
    "\n",
    "Questo tipo di valutazione è ideale:\n",
    "\n",
    "* in fase di test iniziale,\n",
    "* per valutare piccoli dataset,\n",
    "* durante lo sviluppo di RAG specialistici (es. legali, medici, aziendali).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Valutazione Automatica (LLM-as-a-Judge)**\n",
    "\n",
    "Per validare molte risposte rapidamente, è possibile **usare un altro LLM per giudicare** la correttezza dell’output generato dal primo modello.\n",
    "Questa tecnica è chiamata **\"LLM-as-a-Judge\"**.\n",
    "\n",
    "#### Prompt tipico:\n",
    "\n",
    "```text\n",
    "Hai ricevuto una risposta a una domanda, e un insieme di documenti come contesto.\n",
    "\n",
    "Domanda:\n",
    "{question}\n",
    "\n",
    "Contesto:\n",
    "{retrieved_chunks}\n",
    "\n",
    "Risposta:\n",
    "{generated_answer}\n",
    "\n",
    "La risposta è corretta e basata sui documenti?\n",
    "- Rispondi con: CORRETTA / PARZIALMENTE CORRETTA / SBAGLIATA\n",
    "- Spiega brevemente il perché.\n",
    "```\n",
    "\n",
    "#### Output desiderato:\n",
    "\n",
    "```text\n",
    "CORRETTA\n",
    "\n",
    "La risposta specifica l’aliquota IRPEF del 5% come riportato nel documento 1, ed è coerente con le fonti.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Pro e contro della valutazione automatica**\n",
    "\n",
    "| Pro                                    | Contro                                 |\n",
    "| -------------------------------------- | -------------------------------------- |\n",
    "| Rapida e scalabile                     | Meno affidabile per risposte complesse |\n",
    "| Può essere integrata in pipeline MLOps | Rischio di bias del modello            |\n",
    "| Può generare spiegazioni per debugging | Richiede prompt ben progettati         |\n",
    "\n",
    "#### Consigli:\n",
    "\n",
    "* Usa **modelli di valutazione diversi** dal generatore, se possibile.\n",
    "* Usa **zero-shot** o **few-shot prompting** per aiutare il valutatore a essere più rigoroso.\n",
    "* Valuta campioni casuali con **validazione incrociata umana**, per verificare la qualità della valutazione automatica.\n",
    "\n",
    "---\n",
    "\n",
    "### **Bonus: metriche automatiche classiche (limitate)**\n",
    "\n",
    "* **BLEU**, **ROUGE**, **METEOR**: usate nel NLP classico per confrontare output testuali con ground truth.\n",
    "  Ma in RAG sono spesso **inadeguate**, perché la risposta può essere corretta anche se diversa da quella “attesa”.\n",
    "* **BERTScore**: confronta le risposte a livello semantico, usando embedding. Utile per valutazioni più “flessibili”.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "Una buona valutazione delle risposte deve bilanciare:\n",
    "\n",
    "* **accuratezza umana**, quando possibile,\n",
    "* **scalabilità automatica**, con prompt ben calibrati.\n",
    "\n",
    "In un progetto didattico o aziendale, una pipeline mista (es. 10% valutazione umana + 90% automatica) è spesso la soluzione più efficace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0267112a-ee81-4835-837b-aa9a1f7e3f5f",
   "metadata": {},
   "source": [
    "#### **10.3. Prompt di validazione**\n",
    "\n",
    "Oltre a una semplice classificazione “corretta/sbagliata”, i **prompt di validazione ben progettati** possono aiutare un LLM (o un valutatore umano assistito) a giudicare in modo più sottile e preciso le risposte di un sistema RAG.\n",
    "Questa pratica è fondamentale per analizzare: **qualità, affidabilità, tono, completezza, presenza di bias**, e allucinazioni.\n",
    "\n",
    "---\n",
    "\n",
    "### **Prompt per analisi della correttezza**\n",
    "\n",
    "Questi prompt servono per validare se la **risposta è supportata dai documenti** e **rilevante per la domanda**.\n",
    "\n",
    "#### Prompt base:\n",
    "\n",
    "```text\n",
    "Hai ricevuto una domanda, una risposta generata da un modello, e i documenti di supporto.\n",
    "\n",
    "Valuta se la risposta è:\n",
    "1. Corretta rispetto alla domanda\n",
    "2. Basata esclusivamente sui documenti forniti\n",
    "3. Rilevante e focalizzata\n",
    "\n",
    "Domanda: {question}\n",
    "Risposta: {generated_answer}\n",
    "Documenti: {retrieved_documents}\n",
    "\n",
    "Scrivi: CORRETTA / PARZIALMENTE CORRETTA / SBAGLIATA\n",
    "Motiva la tua valutazione in 1-2 frasi.\n",
    "```\n",
    "\n",
    "#### Prompt con scoring:\n",
    "\n",
    "```text\n",
    "Valuta la qualità della risposta da 1 a 5 secondo questi criteri:\n",
    "- Correttezza\n",
    "- Aderenza alle fonti\n",
    "- Completezza\n",
    "- Chiarezza\n",
    "\n",
    "Domanda: {question}\n",
    "Risposta: {generated_answer}\n",
    "Fonti: {retrieved_chunks}\n",
    "\n",
    "Restituisci il punteggio per ogni criterio + una spiegazione breve.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Prompt per valutare tono, bias e completezza**\n",
    "\n",
    "Oltre alla correttezza, in ambienti professionali (legali, medici, aziendali), è spesso utile controllare altri aspetti:\n",
    "\n",
    "#### 1. **Valutazione del tono**\n",
    "\n",
    "```text\n",
    "Analizza la risposta seguente.\n",
    "\n",
    "- È scritta in modo professionale?\n",
    "- È adatta a un pubblico non esperto?\n",
    "- Il tono è neutro, cordiale, autorevole?\n",
    "\n",
    "Domanda: {question}\n",
    "Risposta: {answer}\n",
    "\n",
    "Valuta il tono con: OTTIMALE / ACCETTABILE / NON APPROPRIATO + motivazione.\n",
    "```\n",
    "\n",
    "#### 2. **Rilevamento di bias**\n",
    "\n",
    "```text\n",
    "La risposta contiene bias impliciti o opinioni non supportate?\n",
    "\n",
    "- Ci sono affermazioni soggettive non giustificate?\n",
    "- Viene trattato un argomento sensibile in modo imparziale?\n",
    "\n",
    "Risposta: {answer}\n",
    "Fonti: {retrieved_documents}\n",
    "\n",
    "Restituisci: NEUTRA / PARZIALMENTE BIASATA / BIASATA + spiegazione.\n",
    "```\n",
    "\n",
    "#### 3. **Analisi della completezza**\n",
    "\n",
    "```text\n",
    "La risposta copre tutti i punti chiave richiesti dalla domanda?\n",
    "\n",
    "Domanda: {question}\n",
    "Risposta: {generated_answer}\n",
    "Documenti: {retrieved_chunks}\n",
    "\n",
    "Restituisci: COMPLETA / PARZIALE / INCOMPLETA\n",
    "+ elenco di elementi coperti e mancanti.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Prompt specializzati (per analisi comparativa)**\n",
    "\n",
    "Quando si confrontano più modelli o versioni:\n",
    "\n",
    "```text\n",
    "Hai due risposte alla stessa domanda, generate da due sistemi diversi.\n",
    "\n",
    "- Quale delle due è più completa?\n",
    "- Quale è più corretta secondo i documenti?\n",
    "- Quale è scritta meglio?\n",
    "\n",
    "Domanda: {question}\n",
    "Risposta A: {answer_A}\n",
    "Risposta B: {answer_B}\n",
    "Fonti: {context}\n",
    "\n",
    "Rispondi indicando la preferita e motivando la scelta.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusione**\n",
    "\n",
    "I prompt di validazione ben progettati permettono di:\n",
    "\n",
    "* Automatizzare test complessi,\n",
    "* Identificare debolezze ricorrenti (allucinazioni, omissioni, incoerenza),\n",
    "* Integrare controlli di qualità nelle pipeline di sviluppo o in ambienti di produzione (es. AI assistant, chatbot aziendali, motori di ricerca semantici).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05f2027-3c32-4c84-823e-fee063c40fe6",
   "metadata": {},
   "source": [
    "## RECAP GUIDA ##\n",
    "\n",
    "---\n",
    "\n",
    "# RAG: guida step-by-step (da 0 a prod)\n",
    "\n",
    "```\n",
    "[0] Setup  →  [1] Load  →  [2] Chunk  →  [3] Embed  →  [4] Index\n",
    "            → [5] Retrieve  →  [6] Hybrid  →  [7] Memory\n",
    "            → [8] Prompt+LLM  →  [9] Chain  →  [10] Eval\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Setup ambiente\n",
    "\n",
    "**Install**\n",
    "\n",
    "```bash\n",
    "pip install langchain langchain-community langchain-openai faiss-cpu unstructured pypdf\n",
    "# per HF embeddings:\n",
    "pip install sentence-transformers\n",
    "# opzionale: qdrant-client pinecone-client chromadb\n",
    "```\n",
    "\n",
    "**Chiavi**\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Document loading (`Document`, loader)\n",
    "\n",
    "**Classi chiave**\n",
    "\n",
    "* `langchain.schema.Document`\n",
    "* `DirectoryLoader`, `TextLoader`, `PyPDFLoader`, `UnstructuredPDFLoader`\n",
    "\n",
    "**Esempio**\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "\n",
    "pdf_loader  = DirectoryLoader(\"data/pdf\",  glob=\"**/*.pdf\",  loader_cls=PyPDFLoader)\n",
    "txt_loader  = DirectoryLoader(\"data/txt\",  glob=\"**/*.txt\",  loader_cls=TextLoader)\n",
    "\n",
    "documents = pdf_loader.load() + txt_loader.load()\n",
    "# documents: List[Document] con .page_content e .metadata\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Chunking (splitter)\n",
    "\n",
    "**Classi chiave**\n",
    "\n",
    "* `RecursiveCharacterTextSplitter` (generico)\n",
    "* `MarkdownHeaderTextSplitter` (per .md)\n",
    "* Custom `TextSplitter` (se servono regole ad hoc)\n",
    "\n",
    "**Esempio**\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "chunks = splitter.split_documents(documents)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Embeddings (query & docs nello stesso spazio)\n",
    "\n",
    "**Modelli comuni**\n",
    "\n",
    "* OpenAI: `text-embedding-ada-002` (1536-d)\n",
    "* HF locale: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "* Altri: `BAAI/bge-base-en-v1.5`, `hkunlp/instructor-large`\n",
    "\n",
    "**Classi chiave**\n",
    "\n",
    "* `OpenAIEmbeddings`\n",
    "* `HuggingFaceEmbeddings`\n",
    "\n",
    "**Esempio (OpenAI)**\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "```\n",
    "\n",
    "**Esempio (HF)**\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Vector store (index)\n",
    "\n",
    "**Opzioni**\n",
    "\n",
    "* Locale: `FAISS`, `Chroma`\n",
    "* Self-hosted: `Qdrant`, `Weaviate`\n",
    "* Cloud: `Pinecone`\n",
    "\n",
    "**Esempio (FAISS)**\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "vs = FAISS.from_documents(chunks, emb)       # build\n",
    "vs.save_local(\"index_faiss\")                 # persist\n",
    "# reload: vs = FAISS.load_local(\"index_faiss\", emb)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Retriever (semantic search)\n",
    "\n",
    "**API chiave**\n",
    "\n",
    "* `vectorstore.as_retriever(search_kwargs={\"k\": K, ...})`\n",
    "* `retriever.get_relevant_documents(query)`\n",
    "\n",
    "**Esempio**\n",
    "\n",
    "```python\n",
    "retriever = vs.as_retriever(search_kwargs={\"k\": 4})\n",
    "docs = retriever.get_relevant_documents(\"Come attivo la garanzia?\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Hybrid search (BM25 + vectors)\n",
    "\n",
    "**Classi chiave**\n",
    "\n",
    "* `BM25Retriever.from_documents(chunks)`\n",
    "* `EnsembleRetriever(retrievers=[...], weights=[...])`\n",
    "\n",
    "**Esempio**\n",
    "\n",
    "```python\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "bm25 = BM25Retriever.from_documents(chunks); bm25.k = 4\n",
    "vec  = vs.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "hybrid = EnsembleRetriever(\n",
    "    retrievers=[bm25, vec],\n",
    "    weights=[0.4, 0.6]  # tuning: 0.3–0.7, 0.5–0.5, ecc.\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Conversational memory (token-aware)\n",
    "\n",
    "**Classi chiave**\n",
    "\n",
    "* `ConversationBufferMemory` (storico completo)\n",
    "* `ConversationSummaryMemory` (riassunto automatico)\n",
    "* `ConversationBufferWindowMemory(k=...)` (ultimi N turni)\n",
    "* `VectorStoreRetrieverMemory` (memoria semantica)\n",
    "\n",
    "**Esempio (summary)**\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=ChatOpenAI(temperature=0))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Prompt dinamico (istruzioni + contesto + domanda)\n",
    "\n",
    "**Classi chiave**\n",
    "\n",
    "* `PromptTemplate` / `ChatPromptTemplate`\n",
    "\n",
    "**Esempio**\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "TEMPLATE = \"\"\"Sei un assistente tecnico. Usa SOLO i documenti forniti.\n",
    "Se non trovi la risposta, dì esplicitamente che non è presente.\n",
    "\n",
    "Contesto:\n",
    "{context}\n",
    "\n",
    "Domanda:\n",
    "{question}\n",
    "\n",
    "Rispondi in modo conciso e cita le fonti (metadata.source/pagina).\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=TEMPLATE\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Chain RAG (LLM + retriever)\n",
    "\n",
    "**Classi chiave**\n",
    "\n",
    "* `RetrievalQA.from_chain_type(...)`\n",
    "* Alternative: `ConversationalRetrievalChain` (se chat multi-turno)\n",
    "* Opzione: `return_source_documents=True`\n",
    "\n",
    "**Esempio (RetrievalQA + hybrid)**\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)  # deterministico\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=hybrid,                 # o retriever vettoriale\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "res = qa(\"Qual è la durata della garanzia standard?\")\n",
    "print(res[\"result\"])\n",
    "for d in res[\"source_documents\"]:\n",
    "    print(d.metadata.get(\"source\"), d.metadata.get(\"page\"))\n",
    "```\n",
    "\n",
    "**Conversational (memoria)**\n",
    "\n",
    "```python\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=hybrid,\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n",
    "out = crc({\"question\": \"E per i prodotti ricondizionati?\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Valutazione (retrieval + risposta)\n",
    "\n",
    "### 10.1 Retrieval: **precision\\@k**\n",
    "\n",
    "Pseudo-codice:\n",
    "\n",
    "```python\n",
    "def precision_at_k(queries, gold_doc_ids, retriever, k=5):\n",
    "    ok, tot = 0, 0\n",
    "    for q in queries:\n",
    "        got = retriever.get_relevant_documents(q)[:k]\n",
    "        got_ids = { (d.metadata[\"source\"], d.metadata.get(\"page\")) for d in got }\n",
    "        rel_ids = set(gold_doc_ids[q])  # ground truth\n",
    "        ok += len(got_ids & rel_ids)\n",
    "        tot += k\n",
    "    return ok / tot\n",
    "```\n",
    "\n",
    "### 10.2 Risposta: **LLM-as-a-Judge**\n",
    "\n",
    "Prompt tipo:\n",
    "\n",
    "```text\n",
    "Domanda: {question}\n",
    "Risposta: {answer}\n",
    "Documenti: {chunks}\n",
    "\n",
    "Valuta: CORRETTA / PARZIALE / SBAGLIATA.\n",
    "Spiega in 1-2 frasi citando i documenti.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Scelte rapide (riassunto design)\n",
    "\n",
    "* **Loader**: `PyPDFLoader`/`UnstructuredPDFLoader` + `DirectoryLoader`\n",
    "* **Splitter**: `RecursiveCharacterTextSplitter(500, overlap=100)`\n",
    "* **Embeddings**: `OpenAIEmbeddings(\"text-embedding-ada-002\")` oppure `HuggingFaceEmbeddings(\"all-MiniLM-L6-v2\")`\n",
    "* **Store**: `FAISS` (prototipi) → `Qdrant`/`Pinecone` (prod)\n",
    "* **Retriever**: `vs.as_retriever(k=4)` + opzionale `BM25Retriever` → `EnsembleRetriever`\n",
    "* **Memory**: `ConversationSummaryMemory` per sessioni lunghe\n",
    "* **LLM**: `ChatOpenAI(temperature=0)`\n",
    "* **Chain**: `RetrievalQA(..., return_source_documents=True)`\n",
    "* **Eval**: `precision@k` + **LLM-as-a-Judge**\n",
    "\n",
    "---\n",
    "\n",
    "# Flow minimale “collaudo rapido”\n",
    "\n",
    "```python\n",
    "# 1) load → 2) chunk → 3) embed → 4) index\n",
    "documents = DirectoryLoader(\"data\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader).load()\n",
    "chunks = RecursiveCharacterTextSplitter(500, 100).split_documents(documents)\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vs = FAISS.from_documents(chunks, emb)\n",
    "\n",
    "# 5) retrievers\n",
    "vec  = vs.as_retriever(search_kwargs={\"k\": 4})\n",
    "bm25 = BM25Retriever.from_documents(chunks); bm25.k = 4\n",
    "retr = EnsembleRetriever(retrievers=[bm25, vec], weights=[0.4, 0.6])\n",
    "\n",
    "# 6) chain\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "qa  = RetrievalQA.from_chain_type(llm=llm, retriever=retr, return_source_documents=True)\n",
    "\n",
    "# 7) query\n",
    "res = qa(\"Come si attiva la garanzia del prodotto X?\")\n",
    "print(res[\"result\"])\n",
    "for s in res[\"source_documents\"]:\n",
    "    print(s.metadata)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add09dd-db6f-459c-bb52-91eda3d0b23a",
   "metadata": {},
   "source": [
    "Titolo: Costruisci un RAG end-to-end partendo da documenti generati via prompt\n",
    "\n",
    "Obiettivo: Gli studenti progettano e realizzano un sistema RAG completo. Il corpus di partenza non è fornito: va generato con ChatGPT tramite prompt, in più formati e con diversi stili e livelli di “rumore” (errori, ridondanze, versioni). Al termine, il RAG dovrà rispondere correttamente a domande specifiche citando le fonti.\n",
    "\n",
    "Consegna\n",
    "\n",
    "1. Generazione del corpus via prompt (obbligatoria con ChatGPT)\n",
    "\n",
    "   * Progetta 6–10 prompt per far generare a ChatGPT un corpus multiformato sullo stesso dominio tematico (scegli uno tra: manuale prodotto, regolamento interno, guida API, policy privacy, documentazione corso).\n",
    "   * Deve includere almeno:\n",
    "     a) 2 file .md con titoli gerarchici (H1–H3) e sezioni coerenti;\n",
    "     b) 2 file .txt con FAQ e tutorial passo-passo;\n",
    "     c) 1 file HTML con una sezione “Note legali” e una tabella;\n",
    "     d) 1 CSV con dati tabellari coerenti col dominio (es. tariffario, piani, campi API);\n",
    "     e) 1 documento “noisy” (es. changelog con date/versioni, errori ortografici, sinonimi) in formato .txt;\n",
    "     f) 1 documento lungo che superi 3.000 parole (potrà essere spezzato in più file).\n",
    "   * Variante PDF: prendi uno dei .md generati e convertilo in PDF (anche tramite esportazione del tuo editor) per testare loader diversi.\n",
    "   * Inserisci nei prompt richieste esplicite su: struttura, titoli, metadati nel front-matter (per i .md), esempi concreti, sezioni “Limitazioni/Assunzioni”.\n",
    "\n",
    "   Output atteso: una cartella “data/” con sottocartelle per formato (md/, txt/, html/, csv/, pdf/). Includi un file README che descrive dominio, versione e sintesi dei contenuti.\n",
    "\n",
    "\n",
    "# IN MANCANZA DI TEMPO UTILIZZA SOLO UN TIPO DI FILE!! #\n",
    "3. Caricamento e arricchimento metadati\n",
    "\n",
    "   * Carica tutti i documenti usando loader idonei per ciascun formato.\n",
    "   * Aggiungi metadati minimi: source, format, section/title, version/date (se presenti), language.\n",
    "   * Mostra come useresti filtri sui metadati per restringere la ricerca (es. solo version >= X, solo format=md).\n",
    "\n",
    "4. Chunking strategico\n",
    "\n",
    "   * Applica `RecursiveCharacterTextSplitter` con almeno due configurazioni a confronto (es. 500/100 e 800/120).\n",
    "   * Per i .md usa `MarkdownHeaderTextSplitter` per preservare contesto gerarchico.\n",
    "   * Giustifica per iscritto la configurazione scelta sulla base del dominio e della lunghezza media dei chunk.\n",
    "\n",
    "5. Embedding e indicizzazione\n",
    "\n",
    "   * Confronta due modelli di embedding: uno OpenAI (es. text-embedding-ada-002) e uno locale (es. all-MiniLM-L6-v2 o bge-base).\n",
    "   * Indicizza con FAISS (obbligatorio) e, a scelta, uno tra Qdrant/Chroma/Pinecone.\n",
    "   * Documenta tempi di indicizzazione e dimensione dell’indice.\n",
    "\n",
    "6. Retrieval e ricerca ibrida\n",
    "\n",
    "   * Implementa un retriever vettoriale (`k` tra 3 e 6) e uno BM25.\n",
    "   * Crea un `EnsembleRetriever` con pesi diversi in due scenari (es. 0.4/0.6 e 0.6/0.4). Spiega l’impatto osservato su query con nomi propri, sigle, date/versioni e sinonimi.\n",
    "   * Prepara almeno 12 query di test, di cui: 4 factuali con numeri/date, 4 concettuali, 4 “trick” con ambiguità o sinonimi.\n",
    "\n",
    "7. Prompting e generazione\n",
    "\n",
    "   * Definisci un prompt template: istruzione di sistema + contesto numerato + domanda + richiesta di citare `metadata.source` e, se applicabile, `page`/`section`.\n",
    "   * Usa `RetrievalQA` con `return_source_documents=True`. Imposta `temperature=0`.\n",
    "   * Aggiungi variante conversazionale con `ConversationalRetrievalChain` e `ConversationSummaryMemory` per 3 turni consecutivi sulla stessa tematica.\n",
    "\n",
    "8. Valutazione\n",
    "\n",
    "   * Retrieval: calcola precision\\@k su un sottoinsieme di 10 query con ground truth manuale (indica per ciascuna i documenti attesi).\n",
    "   * Risposta: usa un prompt “LLM-as-a-Judge” per etichettare CORRETTA / PARZIALE / SBAGLIATA + spiegazione di 1–2 frasi, confrontando i due setup di embedding e i due pesi dell’ensemble.\n",
    "   * Riporta una tabella finale con: query, setup, precision\\@k, giudizio risposta, note.\n",
    "\n",
    "9. Analisi errori e miglioramenti\n",
    "\n",
    "   * Identifica 3 cause principali di fallimento (es. chunk troppo lunghi, mancanza di metadati utili, pesi ensemble subottimali).\n",
    "   * Proponi 3 azioni correttive concrete (es. filtro metadati per version, re-ranking lessicale, aumento overlap o split per sezioni).\n",
    "\n",
    "Criteri di valutazione\n",
    "\n",
    "* Completezza del corpus e aderenza ai requisiti di formato e “rumore”.\n",
    "* Qualità del design (scelte motivate su chunking, embedding, retriever).\n",
    "* Correttezza e tracciabilità delle risposte (citazioni fonti).\n",
    "* Rigorosità della valutazione (precision\\@k, LLM-as-a-Judge, analisi comparativa).\n",
    "* Chiarezza del README e del report finale.\n",
    "\n",
    "Consegne richieste\n",
    "\n",
    "* Cartella `data/` con i file generati via ChatGPT e il PDF.\n",
    "* Script o notebook con pipeline completa (load → chunk → embed → index → retrieve → generate → eval).\n",
    "* README con istruzioni d’esecuzione, dipendenze e scelte progettuali.\n",
    "* Report breve (max 2 pagine) con risultati, tabelle di valutazione e proposte di miglioramento.\n",
    "\n",
    "Vincoli\n",
    "\n",
    "* Niente hard-coding delle risposte nel prompt.\n",
    "* Citare sempre le fonti in output (almeno `source`).\n",
    "* Limitare il contesto passato all’LLM a un massimo di 4–6 chunk.\n",
    "\n",
    "Bonus\n",
    "\n",
    "* Re-ranking dei candidati con funzione custom (es. boost su `version` recente).\n",
    "* Integrazione di un piccolo set di test automatizzati.\n",
    "* Visualizzazione interattiva delle query con fonti e score.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
